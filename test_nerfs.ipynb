{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0dbb422",
   "metadata": {},
   "source": [
    "# Toy NeRF Implementation (CPU) - Multi-View\n",
    "\n",
    "This notebook demonstrates a minimal implementation of Neural Radiance Fields (NeRF) designed to run on a CPU.\n",
    "We will train on **multiple** synthetic images to enable basic novel view synthesis.\n",
    "**Note:** Training NeRF models is computationally intensive. This example uses a very small model, limited training iterations, and multiple views, so it will be **very slow** on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f24a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (2.2.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: imageio in /opt/conda/lib/python3.12/site-packages (2.37.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Installation (uncomment if needed)\n",
    "!pip install torch numpy matplotlib tqdm imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c3439a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import imageio.v3 as iio\n",
    "import os\n",
    "\n",
    "# Use GPU is available and CPU otherwise\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d358d7c9",
   "metadata": {},
   "source": [
    "## 2. Ray Generation\n",
    "\n",
    "We need functions to generate rays originating from the camera center and passing through each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2266ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(H, W, focal, c2w):\n",
    "    \"\"\"Generate rays for each pixel in the image.\"\"\"\n",
    "    # Create pixel coordinates\n",
    "    i, j = torch.meshgrid(torch.arange(W, dtype=torch.float32, device=device),\n",
    "                          torch.arange(H, dtype=torch.float32, device=device),\n",
    "                          indexing='xy')\n",
    "\n",
    "    # Convert pixel coordinates to camera coordinates\n",
    "    dirs = torch.stack([(i - W * 0.5) / focal,\n",
    "                        -(j - H * 0.5) / focal,\n",
    "                        -torch.ones_like(i)], dim=-1)\n",
    "\n",
    "    # Rotate directions from camera frame to world frame\n",
    "    rays_d = torch.sum(dirs[..., None, :] * c2w[:3, :3], -1)\n",
    "\n",
    "    # Translate camera frame origin to world frame origin\n",
    "    rays_o = c2w[:3, -1].expand(rays_d.shape)\n",
    "    return rays_o, rays_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae54685",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We'll use a standard synthetic dataset image (e.g., the lego bulldozer). We need the image itself and its corresponding camera pose.\n",
    "\n",
    "**Important:** The original download link is broken. You need to download the data manually:\n",
    "1. Go to the NeRF project page: [https://www.matthewtancik.com/nerf](https://www.matthewtancik.com/nerf)\n",
    "2. Find the 'Data' section and click the link for 'NeRF Synthetic Data'. This will likely lead to a Google Drive folder.\n",
    "3. Download the `nerf_synthetic.zip` file.\n",
    "4. Extract the zip file.\n",
    "5. Place the extracted `lego` folder inside the `nerf_synthetic_lego` directory in the same location as this notebook. You might need to create the `nerf_synthetic_lego` directory first. The final path should look like: `./nerf_synthetic_lego/lego/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b21a1e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Lego dataset...\n",
      "Error downloading dataset: 404 Client Error: Not Found for url: https://cseweb.ucsd.edu//~viscomp/projects/LF/papers/ECCV20/nerf/nerf_synthetic.zip\n",
      "Please download nerf_synthetic.zip manually from the NeRF website and extract the lego folder.\n",
      "Loading the first 20 views...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18344e431eee4d259c070ba090bc13f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resolution: 800x800, Focal length: 1111.11\n",
      "Loaded data shapes:\n",
      "Flattened target shape: torch.Size([12800000, 3])\n",
      "Flattened rays_o shape: torch.Size([12800000, 3])\n",
      "Flattened rays_d shape: torch.Size([12800000, 3])\n"
     ]
    }
   ],
   "source": [
    "# Define the number of views to load\n",
    "num_train_views = 20 # Start with 10 views\n",
    "\n",
    "# Download synthetic NeRF data (Lego bulldozer)\n",
    "if not os.path.exists('nerf_synthetic_lego.zip'):\n",
    "    print('Downloading Lego dataset...')\n",
    "    # Using a known URL for the lego dataset subset\n",
    "    # Note: This requires internet access and might take a moment.\n",
    "    # Consider manually downloading if this fails.\n",
    "    try:\n",
    "        import requests, zipfile, io\n",
    "        url = 'http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/nerf_synthetic.zip'\n",
    "        r = requests.get(url, stream=True)\n",
    "        r.raise_for_status() # Check if the download was successful\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall()\n",
    "        # Rename for clarity\n",
    "        os.rename('nerf_synthetic', 'nerf_synthetic_lego')\n",
    "        print('Download complete.')\n",
    "    except Exception as e:\n",
    "        print(f'Error downloading dataset: {e}')\n",
    "        print('Please download nerf_synthetic.zip manually from the NeRF website and extract the lego folder.')\n",
    "else:\n",
    "    print('Lego dataset already downloaded.')\n",
    "\n",
    "# Load multiple images and their poses\n",
    "data_dir = 'nerf_synthetic_lego/lego'\n",
    "split = 'train'\n",
    "\n",
    "all_target_pixels = []\n",
    "all_rays_o = []\n",
    "all_rays_d = []\n",
    "H, W, focal_length = None, None, None # Initialize these\n",
    "\n",
    "try:\n",
    "    # Load metadata\n",
    "    import json\n",
    "    with open(os.path.join(data_dir, f'transforms_{split}.json'), 'r') as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    print(f\"Loading the first {num_train_views} views...\")\n",
    "    for i in tqdm(range(num_train_views)):\n",
    "        frame = meta['frames'][i]\n",
    "        img_path = os.path.join(data_dir, frame['file_path'] + '.png')\n",
    "        target_img = iio.imread(img_path)\n",
    "        target_img = torch.tensor(target_img[..., :3] / 255.0, dtype=torch.float32).to(device) # Keep only RGB, normalize\n",
    "\n",
    "        pose = torch.tensor(frame['transform_matrix'], dtype=torch.float32).to(device)\n",
    "\n",
    "        if H is None: # Get intrinsics from the first image\n",
    "            H, W = target_img.shape[:2]\n",
    "            camera_angle_x = float(meta['camera_angle_x'])\n",
    "            focal_length = 0.5 * W / np.tan(0.5 * camera_angle_x)\n",
    "            print(f'Image resolution: {H}x{W}, Focal length: {focal_length:.2f}')\n",
    "\n",
    "        # Generate rays for the current view\n",
    "        rays_o, rays_d = get_rays(H, W, focal_length, pose)\n",
    "\n",
    "        # Append flattened data\n",
    "        all_target_pixels.append(target_img.reshape(-1, 3))\n",
    "        all_rays_o.append(rays_o.reshape(-1, 3))\n",
    "        all_rays_d.append(rays_d.reshape(-1, 3))\n",
    "\n",
    "    # Concatenate all data\n",
    "    target_img_flat = torch.cat(all_target_pixels, dim=0)\n",
    "    rays_o_flat = torch.cat(all_rays_o, dim=0)\n",
    "    rays_d_flat = torch.cat(all_rays_d, dim=0)\n",
    "\n",
    "    print(f'Loaded data shapes:')\n",
    "    print(f'Flattened target shape: {target_img_flat.shape}') # Should be (num_views*H*W, 3)\n",
    "    print(f'Flattened rays_o shape: {rays_o_flat.shape}') # Should be (num_views*H*W, 3)\n",
    "    print(f'Flattened rays_d shape: {rays_d_flat.shape}') # Should be (num_views*H*W, 3)\n",
    "\n",
    "    # Prepare data indices for batching\n",
    "    num_rays = rays_o_flat.shape[0]\n",
    "    data_loaded_successfully = True # Flag to indicate success\n",
    "\n",
    "except FileNotFoundError:\n",
    "     print(f'Error: Could not find dataset files in {data_dir}. Ensure the dataset was downloaded and extracted correctly.')\n",
    "     data_loaded_successfully = False\n",
    "except NameError as e:\n",
    "     print(f'Error: Make sure functions like get_rays are defined before this cell. {e}')\n",
    "     data_loaded_successfully = False\n",
    "except Exception as e:\n",
    "     print(f'An error occurred during data loading: {e}')\n",
    "     data_loaded_successfully = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de8fda",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5abbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecd6bf0e",
   "metadata": {},
   "source": [
    "## 3. Positional Encoding\n",
    "\n",
    "NeRF uses positional encoding to help the MLP represent high-frequency details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67ad6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(x, num_freqs):\n",
    "    \"\"\"Apply positional encoding to input tensor.\"\"\"\n",
    "    freq_bands = 2.0**torch.arange(num_freqs, device=device) * torch.pi\n",
    "    scaled_x = x[..., None] * freq_bands\n",
    "    encoded = torch.cat([torch.sin(scaled_x), torch.cos(scaled_x)], dim=-1)\n",
    "    encoded = encoded.reshape(*x.shape[:-1], -1)\n",
    "    return torch.cat([x, encoded], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dfebde",
   "metadata": {},
   "source": [
    "## 4. NeRF Model (MLP)\n",
    "\n",
    "Define the simple Multi-Layer Perceptron (MLP) that forms the core of NeRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ea2d7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyNeRF(\n",
      "  (layer1): Linear(in_features=63, out_features=128, bias=True)\n",
      "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (layer3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (layer4): Linear(in_features=128, out_features=129, bias=True)\n",
      "  (layer5): Linear(in_features=155, out_features=64, bias=True)\n",
      "  (layer6): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TinyNeRF(nn.Module):\n",
    "    def __init__(self, pos_dim=3, dir_dim=3, num_pos_freqs=10, num_dir_freqs=4, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.pos_dim = pos_dim\n",
    "        self.dir_dim = dir_dim\n",
    "        self.num_pos_freqs = num_pos_freqs\n",
    "        self.num_dir_freqs = num_dir_freqs\n",
    "\n",
    "        self.encoded_pos_dim = pos_dim * (1 + 2 * num_pos_freqs)\n",
    "        self.encoded_dir_dim = dir_dim * (1 + 2 * num_dir_freqs)\n",
    "\n",
    "        self.layer1 = nn.Linear(self.encoded_pos_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer4 = nn.Linear(hidden_dim, hidden_dim + 1)\n",
    "\n",
    "        self.layer5 = nn.Linear(hidden_dim + self.encoded_dir_dim, hidden_dim // 2)\n",
    "        self.layer6 = nn.Linear(hidden_dim // 2, 3)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, pos, view_dir):\n",
    "        encoded_pos = positional_encoding(pos, self.num_pos_freqs)\n",
    "        encoded_dir = positional_encoding(view_dir, self.num_dir_freqs)\n",
    "\n",
    "        x = self.relu(self.layer1(encoded_pos))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        features = x[..., :-1]\n",
    "        sigma = self.relu(x[..., -1])\n",
    "\n",
    "        x = torch.cat([features, encoded_dir], dim=-1)\n",
    "        x = self.relu(self.layer5(x))\n",
    "        rgb = self.sigmoid(self.layer6(x))\n",
    "\n",
    "        return rgb, sigma\n",
    "\n",
    "# Instantiate the model\n",
    "model = TinyNeRF(hidden_dim=128).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b33ab",
   "metadata": {},
   "source": [
    "## 5. Volume Rendering\n",
    "\n",
    "Implement the volume rendering equation to composite colors and densities along each ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b648bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rays(model, rays_o, rays_d, near=2.0, far=6.0, num_samples=64):\n",
    "    t_vals = torch.linspace(near, far, num_samples, device=device)\n",
    "    points = rays_o[..., None, :] + rays_d[..., None, :] * t_vals[..., :, None]\n",
    "    view_dirs = rays_d[..., None, :].expand(-1, num_samples, -1)\n",
    "\n",
    "    points_flat = points.reshape(-1, 3)\n",
    "    view_dirs_flat = view_dirs.reshape(-1, 3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rgb_flat, sigma_flat = model(points_flat, view_dirs_flat)\n",
    "\n",
    "    rgb = rgb_flat.reshape(*points.shape[:-1], 3)\n",
    "    sigma = sigma_flat.reshape(*points.shape[:-1])\n",
    "\n",
    "    diffs = t_vals[1:] - t_vals[:-1]\n",
    "    dists_1d = torch.cat([diffs, torch.tensor([1e10], device=device)], dim=0)\n",
    "    dists = dists_1d.expand(sigma.shape)\n",
    "\n",
    "    alpha = 1.0 - torch.exp(-sigma * dists)\n",
    "    transmittance = torch.cumprod(torch.cat([torch.ones_like(alpha[:, :1]), 1.0 - alpha + 1e-10], dim=-1), dim=-1)[:, :-1]\n",
    "    weights = transmittance * alpha\n",
    "\n",
    "    rendered_color = torch.sum(weights[..., None] * rgb, dim=-2)\n",
    "\n",
    "    return rendered_color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c412e81",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "Set up the optimizer and run the training loop. We'll train on batches of rays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfe5abdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 36000 iterations on 20 views...\n",
      "Total rays: 12800000, Batch size: 1024\n",
      "Saving checkpoints every 1000 iterations.\n",
      "This will be significantly slower than single-view training on CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b925edbdfda432f90e86e1c9d045cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/36000, Loss: 0.073295\n",
      "Iteration 200/36000, Loss: 0.079284\n",
      "Iteration 300/36000, Loss: 0.081681\n",
      "Iteration 400/36000, Loss: 0.085783\n",
      "Iteration 500/36000, Loss: 0.079998\n",
      "Iteration 600/36000, Loss: 0.074063\n",
      "Iteration 700/36000, Loss: 0.085740\n",
      "Iteration 800/36000, Loss: 0.081126\n",
      "Iteration 900/36000, Loss: 0.077434\n",
      "Iteration 1000/36000, Loss: 0.081365\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_1000iter_cuda.pth\n",
      "Iteration 1100/36000, Loss: 0.087175\n",
      "Iteration 1200/36000, Loss: 0.077589\n",
      "Iteration 1300/36000, Loss: 0.084480\n",
      "Iteration 1400/36000, Loss: 0.077347\n",
      "Iteration 1500/36000, Loss: 0.080689\n",
      "Iteration 1600/36000, Loss: 0.073715\n",
      "Iteration 1700/36000, Loss: 0.086437\n",
      "Iteration 1800/36000, Loss: 0.078164\n",
      "Iteration 1900/36000, Loss: 0.081656\n",
      "Iteration 2000/36000, Loss: 0.080883\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_2000iter_cuda.pth\n",
      "Iteration 2100/36000, Loss: 0.083465\n",
      "Iteration 2200/36000, Loss: 0.084936\n",
      "Iteration 2300/36000, Loss: 0.080717\n",
      "Iteration 2400/36000, Loss: 0.083176\n",
      "Iteration 2500/36000, Loss: 0.082057\n",
      "Iteration 2600/36000, Loss: 0.086231\n",
      "Iteration 2700/36000, Loss: 0.086571\n",
      "Iteration 2800/36000, Loss: 0.075091\n",
      "Iteration 2900/36000, Loss: 0.085315\n",
      "Iteration 3000/36000, Loss: 0.086759\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_3000iter_cuda.pth\n",
      "Iteration 3100/36000, Loss: 0.072416\n",
      "Iteration 3200/36000, Loss: 0.076313\n",
      "Iteration 3300/36000, Loss: 0.086030\n",
      "Iteration 3400/36000, Loss: 0.084657\n",
      "Iteration 3500/36000, Loss: 0.076286\n",
      "Iteration 3600/36000, Loss: 0.080946\n",
      "Iteration 3700/36000, Loss: 0.081605\n",
      "Iteration 3800/36000, Loss: 0.074871\n",
      "Iteration 3900/36000, Loss: 0.086117\n",
      "Iteration 4000/36000, Loss: 0.079699\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_4000iter_cuda.pth\n",
      "Iteration 4100/36000, Loss: 0.079241\n",
      "Iteration 4200/36000, Loss: 0.082387\n",
      "Iteration 4300/36000, Loss: 0.082175\n",
      "Iteration 4400/36000, Loss: 0.087952\n",
      "Iteration 4500/36000, Loss: 0.073230\n",
      "Iteration 4600/36000, Loss: 0.076857\n",
      "Iteration 4700/36000, Loss: 0.085754\n",
      "Iteration 4800/36000, Loss: 0.083862\n",
      "Iteration 4900/36000, Loss: 0.083837\n",
      "Iteration 5000/36000, Loss: 0.090381\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_5000iter_cuda.pth\n",
      "Iteration 5100/36000, Loss: 0.080580\n",
      "Iteration 5200/36000, Loss: 0.091975\n",
      "Iteration 5300/36000, Loss: 0.074490\n",
      "Iteration 5400/36000, Loss: 0.081047\n",
      "Iteration 5500/36000, Loss: 0.088853\n",
      "Iteration 5600/36000, Loss: 0.082512\n",
      "Iteration 5700/36000, Loss: 0.079936\n",
      "Iteration 5800/36000, Loss: 0.087063\n",
      "Iteration 5900/36000, Loss: 0.082954\n",
      "Iteration 6000/36000, Loss: 0.073207\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_6000iter_cuda.pth\n",
      "Iteration 6100/36000, Loss: 0.081660\n",
      "Iteration 6200/36000, Loss: 0.083929\n",
      "Iteration 6300/36000, Loss: 0.088959\n",
      "Iteration 6400/36000, Loss: 0.090912\n",
      "Iteration 6500/36000, Loss: 0.084703\n",
      "Iteration 6600/36000, Loss: 0.086683\n",
      "Iteration 6700/36000, Loss: 0.087900\n",
      "Iteration 6800/36000, Loss: 0.078120\n",
      "Iteration 6900/36000, Loss: 0.075625\n",
      "Iteration 7000/36000, Loss: 0.090685\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_7000iter_cuda.pth\n",
      "Iteration 7100/36000, Loss: 0.078751\n",
      "Iteration 7200/36000, Loss: 0.067889\n",
      "Iteration 7300/36000, Loss: 0.086180\n",
      "Iteration 7400/36000, Loss: 0.079378\n",
      "Iteration 7500/36000, Loss: 0.083816\n",
      "Iteration 7600/36000, Loss: 0.077193\n",
      "Iteration 7700/36000, Loss: 0.080767\n",
      "Iteration 7800/36000, Loss: 0.090359\n",
      "Iteration 7900/36000, Loss: 0.083665\n",
      "Iteration 8000/36000, Loss: 0.076167\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_8000iter_cuda.pth\n",
      "Iteration 8100/36000, Loss: 0.078329\n",
      "Iteration 8200/36000, Loss: 0.081645\n",
      "Iteration 8300/36000, Loss: 0.075098\n",
      "Iteration 8400/36000, Loss: 0.072458\n",
      "Iteration 8500/36000, Loss: 0.083771\n",
      "Iteration 8600/36000, Loss: 0.076258\n",
      "Iteration 8700/36000, Loss: 0.082359\n",
      "Iteration 8800/36000, Loss: 0.074996\n",
      "Iteration 8900/36000, Loss: 0.072459\n",
      "Iteration 9000/36000, Loss: 0.080953\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_9000iter_cuda.pth\n",
      "Iteration 9100/36000, Loss: 0.083517\n",
      "Iteration 9200/36000, Loss: 0.079953\n",
      "Iteration 9300/36000, Loss: 0.075228\n",
      "Iteration 9400/36000, Loss: 0.071671\n",
      "Iteration 9500/36000, Loss: 0.080723\n",
      "Iteration 9600/36000, Loss: 0.078383\n",
      "Iteration 9700/36000, Loss: 0.079341\n",
      "Iteration 9800/36000, Loss: 0.077801\n",
      "Iteration 9900/36000, Loss: 0.078255\n",
      "Iteration 10000/36000, Loss: 0.089354\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_10000iter_cuda.pth\n",
      "Iteration 10100/36000, Loss: 0.085962\n",
      "Iteration 10200/36000, Loss: 0.083601\n",
      "Iteration 10300/36000, Loss: 0.083488\n",
      "Iteration 10400/36000, Loss: 0.079675\n",
      "Iteration 10500/36000, Loss: 0.090588\n",
      "Iteration 10600/36000, Loss: 0.079891\n",
      "Iteration 10700/36000, Loss: 0.072227\n",
      "Iteration 10800/36000, Loss: 0.087775\n",
      "Iteration 10900/36000, Loss: 0.081545\n",
      "Iteration 11000/36000, Loss: 0.072182\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_11000iter_cuda.pth\n",
      "Iteration 11100/36000, Loss: 0.077049\n",
      "Iteration 11200/36000, Loss: 0.081932\n",
      "Iteration 11300/36000, Loss: 0.081843\n",
      "Iteration 11400/36000, Loss: 0.088104\n",
      "Iteration 11500/36000, Loss: 0.080794\n",
      "Iteration 11600/36000, Loss: 0.081528\n",
      "Iteration 11700/36000, Loss: 0.077358\n",
      "Iteration 11800/36000, Loss: 0.068575\n",
      "Iteration 11900/36000, Loss: 0.096781\n",
      "Iteration 12000/36000, Loss: 0.076512\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_12000iter_cuda.pth\n",
      "Iteration 12100/36000, Loss: 0.085500\n",
      "Iteration 12200/36000, Loss: 0.076767\n",
      "Iteration 12300/36000, Loss: 0.082523\n",
      "Iteration 12400/36000, Loss: 0.084676\n",
      "Iteration 12500/36000, Loss: 0.085087\n",
      "Iteration 12600/36000, Loss: 0.088561\n",
      "Iteration 12700/36000, Loss: 0.079806\n",
      "Iteration 12800/36000, Loss: 0.077229\n",
      "Iteration 12900/36000, Loss: 0.078947\n",
      "Iteration 13000/36000, Loss: 0.086993\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_13000iter_cuda.pth\n",
      "Iteration 13100/36000, Loss: 0.090931\n",
      "Iteration 13200/36000, Loss: 0.086122\n",
      "Iteration 13300/36000, Loss: 0.085470\n",
      "Iteration 13400/36000, Loss: 0.072249\n",
      "Iteration 13500/36000, Loss: 0.085329\n",
      "Iteration 13600/36000, Loss: 0.073418\n",
      "Iteration 13700/36000, Loss: 0.083850\n",
      "Iteration 13800/36000, Loss: 0.094237\n",
      "Iteration 13900/36000, Loss: 0.082269\n",
      "Iteration 14000/36000, Loss: 0.087297\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_14000iter_cuda.pth\n",
      "Iteration 14100/36000, Loss: 0.081352\n",
      "Iteration 14200/36000, Loss: 0.080461\n",
      "Iteration 14300/36000, Loss: 0.073774\n",
      "Iteration 14400/36000, Loss: 0.068657\n",
      "Iteration 14500/36000, Loss: 0.092881\n",
      "Iteration 14600/36000, Loss: 0.077702\n",
      "Iteration 14700/36000, Loss: 0.078445\n",
      "Iteration 14800/36000, Loss: 0.081926\n",
      "Iteration 14900/36000, Loss: 0.087241\n",
      "Iteration 15000/36000, Loss: 0.081340\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_15000iter_cuda.pth\n",
      "Iteration 15100/36000, Loss: 0.087918\n",
      "Iteration 15200/36000, Loss: 0.084866\n",
      "Iteration 15300/36000, Loss: 0.080998\n",
      "Iteration 15400/36000, Loss: 0.081276\n",
      "Iteration 15500/36000, Loss: 0.071577\n",
      "Iteration 15600/36000, Loss: 0.078733\n",
      "Iteration 15700/36000, Loss: 0.079196\n",
      "Iteration 15800/36000, Loss: 0.086267\n",
      "Iteration 15900/36000, Loss: 0.080900\n",
      "Iteration 16000/36000, Loss: 0.092312\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_16000iter_cuda.pth\n",
      "Iteration 16100/36000, Loss: 0.085455\n",
      "Iteration 16200/36000, Loss: 0.080707\n",
      "Iteration 16300/36000, Loss: 0.081934\n",
      "Iteration 16400/36000, Loss: 0.074590\n",
      "Iteration 16500/36000, Loss: 0.079987\n",
      "Iteration 16600/36000, Loss: 0.087825\n",
      "Iteration 16700/36000, Loss: 0.084919\n",
      "Iteration 16800/36000, Loss: 0.085266\n",
      "Iteration 16900/36000, Loss: 0.083539\n",
      "Iteration 17000/36000, Loss: 0.091368\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_17000iter_cuda.pth\n",
      "Iteration 17100/36000, Loss: 0.080287\n",
      "Iteration 17200/36000, Loss: 0.082417\n",
      "Iteration 17300/36000, Loss: 0.083773\n",
      "Iteration 17400/36000, Loss: 0.076282\n",
      "Iteration 17500/36000, Loss: 0.075832\n",
      "Iteration 17600/36000, Loss: 0.079280\n",
      "Iteration 17700/36000, Loss: 0.091606\n",
      "Iteration 17800/36000, Loss: 0.086281\n",
      "Iteration 17900/36000, Loss: 0.078208\n",
      "Iteration 18000/36000, Loss: 0.075687\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_18000iter_cuda.pth\n",
      "Iteration 18100/36000, Loss: 0.069808\n",
      "Iteration 18200/36000, Loss: 0.084297\n",
      "Iteration 18300/36000, Loss: 0.091241\n",
      "Iteration 18400/36000, Loss: 0.072906\n",
      "Iteration 18500/36000, Loss: 0.091571\n",
      "Iteration 18600/36000, Loss: 0.080584\n",
      "Iteration 18700/36000, Loss: 0.075293\n",
      "Iteration 18800/36000, Loss: 0.077514\n",
      "Iteration 18900/36000, Loss: 0.084798\n",
      "Iteration 19000/36000, Loss: 0.084234\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_19000iter_cuda.pth\n",
      "Iteration 19100/36000, Loss: 0.083493\n",
      "Iteration 19200/36000, Loss: 0.085595\n",
      "Iteration 19300/36000, Loss: 0.076662\n",
      "Iteration 19400/36000, Loss: 0.072481\n",
      "Iteration 19500/36000, Loss: 0.074946\n",
      "Iteration 19600/36000, Loss: 0.079458\n",
      "Iteration 19700/36000, Loss: 0.085139\n",
      "Iteration 19800/36000, Loss: 0.077973\n",
      "Iteration 19900/36000, Loss: 0.068352\n",
      "Iteration 20000/36000, Loss: 0.081788\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_20000iter_cuda.pth\n",
      "Iteration 20100/36000, Loss: 0.069925\n",
      "Iteration 20200/36000, Loss: 0.091803\n",
      "Iteration 20300/36000, Loss: 0.079049\n",
      "Iteration 20400/36000, Loss: 0.083791\n",
      "Iteration 20500/36000, Loss: 0.081027\n",
      "Iteration 20600/36000, Loss: 0.080903\n",
      "Iteration 20700/36000, Loss: 0.082944\n",
      "Iteration 20800/36000, Loss: 0.080628\n",
      "Iteration 20900/36000, Loss: 0.081685\n",
      "Iteration 21000/36000, Loss: 0.077089\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_21000iter_cuda.pth\n",
      "Iteration 21100/36000, Loss: 0.077742\n",
      "Iteration 21200/36000, Loss: 0.083291\n",
      "Iteration 21300/36000, Loss: 0.083991\n",
      "Iteration 21400/36000, Loss: 0.084723\n",
      "Iteration 21500/36000, Loss: 0.073808\n",
      "Iteration 21600/36000, Loss: 0.079092\n",
      "Iteration 21700/36000, Loss: 0.084791\n",
      "Iteration 21800/36000, Loss: 0.082217\n",
      "Iteration 21900/36000, Loss: 0.079095\n",
      "Iteration 22000/36000, Loss: 0.078328\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_22000iter_cuda.pth\n",
      "Iteration 22100/36000, Loss: 0.075661\n",
      "Iteration 22200/36000, Loss: 0.087597\n",
      "Iteration 22300/36000, Loss: 0.086154\n",
      "Iteration 22400/36000, Loss: 0.075849\n",
      "Iteration 22500/36000, Loss: 0.079832\n",
      "Iteration 22600/36000, Loss: 0.079911\n",
      "Iteration 22700/36000, Loss: 0.080685\n",
      "Iteration 22800/36000, Loss: 0.083726\n",
      "Iteration 22900/36000, Loss: 0.083488\n",
      "Iteration 23000/36000, Loss: 0.093277\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_23000iter_cuda.pth\n",
      "Iteration 23100/36000, Loss: 0.088175\n",
      "Iteration 23200/36000, Loss: 0.078541\n",
      "Iteration 23300/36000, Loss: 0.075669\n",
      "Iteration 23400/36000, Loss: 0.085474\n",
      "Iteration 23500/36000, Loss: 0.085222\n",
      "Iteration 23600/36000, Loss: 0.089516\n",
      "Iteration 23700/36000, Loss: 0.083871\n",
      "Iteration 23800/36000, Loss: 0.084293\n",
      "Iteration 23900/36000, Loss: 0.079620\n",
      "Iteration 24000/36000, Loss: 0.081765\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_24000iter_cuda.pth\n",
      "Iteration 24100/36000, Loss: 0.073888\n",
      "Iteration 24200/36000, Loss: 0.080131\n",
      "Iteration 24300/36000, Loss: 0.087120\n",
      "Iteration 24400/36000, Loss: 0.085780\n",
      "Iteration 24500/36000, Loss: 0.084251\n",
      "Iteration 24600/36000, Loss: 0.087300\n",
      "Iteration 24700/36000, Loss: 0.074160\n",
      "Iteration 24800/36000, Loss: 0.088846\n",
      "Iteration 24900/36000, Loss: 0.079611\n",
      "Iteration 25000/36000, Loss: 0.073087\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_25000iter_cuda.pth\n",
      "Iteration 25100/36000, Loss: 0.072063\n",
      "Iteration 25200/36000, Loss: 0.087727\n",
      "Iteration 25300/36000, Loss: 0.093481\n",
      "Iteration 25400/36000, Loss: 0.081018\n",
      "Iteration 25500/36000, Loss: 0.091055\n",
      "Iteration 25600/36000, Loss: 0.088060\n",
      "Iteration 25700/36000, Loss: 0.077171\n",
      "Iteration 25800/36000, Loss: 0.083097\n",
      "Iteration 25900/36000, Loss: 0.080558\n",
      "Iteration 26000/36000, Loss: 0.082297\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_26000iter_cuda.pth\n",
      "Iteration 26100/36000, Loss: 0.085369\n",
      "Iteration 26200/36000, Loss: 0.072169\n",
      "Iteration 26300/36000, Loss: 0.091527\n",
      "Iteration 26400/36000, Loss: 0.072424\n",
      "Iteration 26500/36000, Loss: 0.085967\n",
      "Iteration 26600/36000, Loss: 0.075040\n",
      "Iteration 26700/36000, Loss: 0.099931\n",
      "Iteration 26800/36000, Loss: 0.077430\n",
      "Iteration 26900/36000, Loss: 0.086990\n",
      "Iteration 27000/36000, Loss: 0.090694\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_27000iter_cuda.pth\n",
      "Iteration 27100/36000, Loss: 0.080937\n",
      "Iteration 27200/36000, Loss: 0.074654\n",
      "Iteration 27300/36000, Loss: 0.080356\n",
      "Iteration 27400/36000, Loss: 0.080102\n",
      "Iteration 27500/36000, Loss: 0.071897\n",
      "Iteration 27600/36000, Loss: 0.089610\n",
      "Iteration 27700/36000, Loss: 0.086104\n",
      "Iteration 27800/36000, Loss: 0.079622\n",
      "Iteration 27900/36000, Loss: 0.083390\n",
      "Iteration 28000/36000, Loss: 0.090793\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_28000iter_cuda.pth\n",
      "Iteration 28100/36000, Loss: 0.086254\n",
      "Iteration 28200/36000, Loss: 0.080559\n",
      "Iteration 28300/36000, Loss: 0.088491\n",
      "Iteration 28400/36000, Loss: 0.083869\n",
      "Iteration 28500/36000, Loss: 0.076130\n",
      "Iteration 28600/36000, Loss: 0.074839\n",
      "Iteration 28700/36000, Loss: 0.081102\n",
      "Iteration 28800/36000, Loss: 0.078224\n",
      "Iteration 28900/36000, Loss: 0.079902\n",
      "Iteration 29000/36000, Loss: 0.082420\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_29000iter_cuda.pth\n",
      "Iteration 29100/36000, Loss: 0.077267\n",
      "Iteration 29200/36000, Loss: 0.076034\n",
      "Iteration 29300/36000, Loss: 0.080392\n",
      "Iteration 29400/36000, Loss: 0.087797\n",
      "Iteration 29500/36000, Loss: 0.082432\n",
      "Iteration 29600/36000, Loss: 0.088523\n",
      "Iteration 29700/36000, Loss: 0.076473\n",
      "Iteration 29800/36000, Loss: 0.082960\n",
      "Iteration 29900/36000, Loss: 0.086191\n",
      "Iteration 30000/36000, Loss: 0.083862\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_30000iter_cuda.pth\n",
      "Iteration 30100/36000, Loss: 0.086628\n",
      "Iteration 30200/36000, Loss: 0.076280\n",
      "Iteration 30300/36000, Loss: 0.083273\n",
      "Iteration 30400/36000, Loss: 0.074207\n",
      "Iteration 30500/36000, Loss: 0.076258\n",
      "Iteration 30600/36000, Loss: 0.082686\n",
      "Iteration 30700/36000, Loss: 0.084363\n",
      "Iteration 30800/36000, Loss: 0.082252\n",
      "Iteration 30900/36000, Loss: 0.082705\n",
      "Iteration 31000/36000, Loss: 0.082978\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_31000iter_cuda.pth\n",
      "Iteration 31100/36000, Loss: 0.076001\n",
      "Iteration 31200/36000, Loss: 0.082667\n",
      "Iteration 31300/36000, Loss: 0.077176\n",
      "Iteration 31400/36000, Loss: 0.086583\n",
      "Iteration 31500/36000, Loss: 0.083736\n",
      "Iteration 31600/36000, Loss: 0.079756\n",
      "Iteration 31700/36000, Loss: 0.083887\n",
      "Iteration 31800/36000, Loss: 0.076132\n",
      "Iteration 31900/36000, Loss: 0.080322\n",
      "Iteration 32000/36000, Loss: 0.082664\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_32000iter_cuda.pth\n",
      "Iteration 32100/36000, Loss: 0.081825\n",
      "Iteration 32200/36000, Loss: 0.080000\n",
      "Iteration 32300/36000, Loss: 0.083105\n",
      "Iteration 32400/36000, Loss: 0.086691\n",
      "Iteration 32500/36000, Loss: 0.087084\n",
      "Iteration 32600/36000, Loss: 0.091084\n",
      "Iteration 32700/36000, Loss: 0.076888\n",
      "Iteration 32800/36000, Loss: 0.078571\n",
      "Iteration 32900/36000, Loss: 0.084117\n",
      "Iteration 33000/36000, Loss: 0.079385\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_33000iter_cuda.pth\n",
      "Iteration 33100/36000, Loss: 0.081484\n",
      "Iteration 33200/36000, Loss: 0.084600\n",
      "Iteration 33300/36000, Loss: 0.078061\n",
      "Iteration 33400/36000, Loss: 0.085033\n",
      "Iteration 33500/36000, Loss: 0.085775\n",
      "Iteration 33600/36000, Loss: 0.071854\n",
      "Iteration 33700/36000, Loss: 0.077563\n",
      "Iteration 33800/36000, Loss: 0.084858\n",
      "Iteration 33900/36000, Loss: 0.079330\n",
      "Iteration 34000/36000, Loss: 0.079787\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_34000iter_cuda.pth\n",
      "Iteration 34100/36000, Loss: 0.069383\n",
      "Iteration 34200/36000, Loss: 0.086655\n",
      "Iteration 34300/36000, Loss: 0.077074\n",
      "Iteration 34400/36000, Loss: 0.076776\n",
      "Iteration 34500/36000, Loss: 0.078833\n",
      "Iteration 34600/36000, Loss: 0.084657\n",
      "Iteration 34700/36000, Loss: 0.089386\n",
      "Iteration 34800/36000, Loss: 0.076154\n",
      "Iteration 34900/36000, Loss: 0.089408\n",
      "Iteration 35000/36000, Loss: 0.084413\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_35000iter_cuda.pth\n",
      "Iteration 35100/36000, Loss: 0.090342\n",
      "Iteration 35200/36000, Loss: 0.075888\n",
      "Iteration 35300/36000, Loss: 0.080976\n",
      "Iteration 35400/36000, Loss: 0.086435\n",
      "Iteration 35500/36000, Loss: 0.083622\n",
      "Iteration 35600/36000, Loss: 0.087755\n",
      "Iteration 35700/36000, Loss: 0.080015\n",
      "Iteration 35800/36000, Loss: 0.083184\n",
      "Iteration 35900/36000, Loss: 0.085123\n",
      "Iteration 36000/36000, Loss: 0.081004\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_36000iter_cuda.pth\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "learning_rate = 5e-4\n",
    "num_iterations = 36000\n",
    "batch_size = 1024\n",
    "display_rate = 100\n",
    "save_rate = 1000 # Save checkpoint every 1000 iterations\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Check if data loaded successfully before starting training\n",
    "if 'data_loaded_successfully' in locals() and data_loaded_successfully:\n",
    "    print(f'Starting training for {num_iterations} iterations on {num_train_views} views...')\n",
    "    print(f'Total rays: {num_rays}, Batch size: {batch_size}')\n",
    "    print(f'Saving checkpoints every {save_rate} iterations.')\n",
    "    print('This will be significantly slower than single-view training on CPU.')\n",
    "\n",
    "    model.train()\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        indices = torch.randint(0, num_rays, (batch_size,))\n",
    "        batch_rays_o = rays_o_flat[indices].to(device)\n",
    "        batch_rays_d = rays_d_flat[indices].to(device)\n",
    "        batch_target_pixels = target_img_flat[indices].to(device)\n",
    "\n",
    "        t_vals = torch.linspace(2.0, 6.0, 64, device=device)\n",
    "        t_rand = torch.rand(batch_size, 64, device=device) * (t_vals[1] - t_vals[0])\n",
    "        t_vals_noisy = t_vals + t_rand\n",
    "\n",
    "        points = batch_rays_o[..., None, :] + batch_rays_d[..., None, :] * t_vals_noisy[..., :, None]\n",
    "        view_dirs = batch_rays_d[..., None, :].expand(-1, 64, -1)\n",
    "\n",
    "        points_flat = points.reshape(-1, 3)\n",
    "        view_dirs_flat = view_dirs.reshape(-1, 3)\n",
    "\n",
    "        rgb_flat, sigma_flat = model(points_flat, view_dirs_flat)\n",
    "\n",
    "        rgb = rgb_flat.reshape(batch_size, 64, 3)\n",
    "        sigma = sigma_flat.reshape(batch_size, 64)\n",
    "\n",
    "        dists = torch.cat([t_vals_noisy[:, 1:] - t_vals_noisy[:, :-1], torch.tensor([1e10], device=device).expand(batch_size, 1)], dim=-1)\n",
    "        alpha = 1.0 - torch.exp(-sigma * dists)\n",
    "        transmittance = torch.cumprod(torch.cat([torch.ones_like(alpha[:, :1]), 1.0 - alpha + 1e-10], dim=-1), dim=-1)[:, :-1]\n",
    "        weights = transmittance * alpha\n",
    "        rendered_color = torch.sum(weights[..., None] * rgb, dim=-2)\n",
    "\n",
    "        loss = mse_loss(rendered_color, batch_target_pixels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % display_rate == 0:\n",
    "            tqdm.write(f'Iteration {i+1}/{num_iterations}, Loss: {loss.item():.6f}')\n",
    "\n",
    "        # --- Checkpoint Saving ---\n",
    "        if (i + 1) % save_rate == 0:\n",
    "            save_num_views_ckpt = num_train_views if 'num_train_views' in locals() else 'multi'\n",
    "            checkpoint_path = f'tiny_nerf_lego_multi_{save_num_views_ckpt}views_{i+1}iter_{device}.pth'\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            tqdm.write(f'Checkpoint saved to {checkpoint_path}')\n",
    "        # --- End Checkpoint Saving ---\n",
    "\n",
    "    print('Training finished.')\n",
    "else:\n",
    "    print('Skipping training because data loading failed in the previous step.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f1453",
   "metadata": {},
   "source": [
    "## 7. Visualization\n",
    "\n",
    "Render the full image using the trained model and compare it to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "701a8be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load test image: 'NoneType' object is not subscriptable\n",
      "Skipping test image rendering due to loading error.\n"
     ]
    }
   ],
   "source": [
    "# Check if training ran before attempting visualization\n",
    "if 'data_loaded_successfully' in locals() and data_loaded_successfully:\n",
    "    model.eval()\n",
    "\n",
    "    # --- Load a test view for evaluation ---\n",
    "    test_split = 'test'\n",
    "    test_img_idx = 0\n",
    "    test_target_img, test_pose = None, None\n",
    "    try:\n",
    "        with open(os.path.join(data_dir, f'transforms_{test_split}.json'), 'r') as f:\n",
    "            test_meta = json.load(f)\n",
    "        test_frame = test_meta['frames'][test_img_idx]\n",
    "        test_img_path = os.path.join(data_dir, test_frame['file_path'] + '.png')\n",
    "        if H is None or W is None or focal_length is None:\n",
    "             raise ValueError(\"Image dimensions or focal length not loaded correctly.\")\n",
    "        test_target_img = torch.tensor(test_target_img[..., :3] / 255.0, dtype=torch.float32).to(device)\n",
    "        test_pose = torch.tensor(test_frame['transform_matrix'], dtype=torch.float32).to(device)\n",
    "        print(f\"Loaded test image {test_img_idx} for evaluation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load test image: {e}\")\n",
    "    # --- End loading test view ---\n",
    "\n",
    "    if test_target_img is not None and test_pose is not None:\n",
    "        test_rays_o, test_rays_d = get_rays(H, W, focal_length, test_pose)\n",
    "        test_rays_o_flat = test_rays_o.reshape(-1, 3)\n",
    "        test_rays_d_flat = test_rays_d.reshape(-1, 3)\n",
    "        test_num_rays = test_rays_o_flat.shape[0]\n",
    "\n",
    "        rendered_image_flat = []\n",
    "        render_batch_size = 2048\n",
    "\n",
    "        print(f'Rendering test image (using {render_batch_size} rays per batch)...')\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, test_num_rays, render_batch_size)):\n",
    "                batch_rays_o = test_rays_o_flat[i:i+render_batch_size].to(device)\n",
    "                batch_rays_d = test_rays_d_flat[i:i+render_batch_size].to(device)\n",
    "                rendered_batch = render_rays(model, batch_rays_o, batch_rays_d, near=2.0, far=6.0, num_samples=64)\n",
    "                rendered_image_flat.append(rendered_batch)\n",
    "\n",
    "        rendered_image = torch.cat(rendered_image_flat, dim=0).reshape(H, W, 3)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        axes[0].imshow(test_target_img.cpu().numpy())\n",
    "        axes[0].set_title(f'Test Image ({test_img_idx})')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(rendered_image.cpu().numpy())\n",
    "        axes[1].set_title(f'Rendered Test Image (After {num_iterations} iterations)')\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        mse = torch.mean((rendered_image - test_target_img)**2)\n",
    "        psnr = -10.0 * torch.log10(mse)\n",
    "        print(f'Test Image PSNR: {psnr.item():.2f} dB')\n",
    "    else:\n",
    "        print(\"Skipping test image rendering due to loading error.\")\n",
    "else:\n",
    "    print('Skipping visualization because data loading or training failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27293c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model weights saved to tiny_nerf_lego_multi_20views_36000iterations_final_cuda.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the final trained model weights\n",
    "if 'data_loaded_successfully' in locals() and data_loaded_successfully:\n",
    "    save_num_views = num_train_views if 'num_train_views' in locals() else 'multi'\n",
    "    model_save_path = f'tiny_nerf_lego_multi_{save_num_views}views_{num_iterations}iterations_final_{device}.pth' # Added _final\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f'Final model weights saved to {model_save_path}')\n",
    "else:\n",
    "    print('Skipping model saving because training did not complete successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a3aa1e",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook provided a minimal NeRF implementation focused on overfitting a single image on the CPU.\n",
    "Key takeaways:\n",
    "*   NeRF combines positional encoding, an MLP, and volume rendering.\n",
    "*   Training even a small model on a single image is slow on CPU.\n",
    "*   The quality depends heavily on model size, number of samples per ray, and training iterations.\n",
    "\n",
    "Further exploration could involve:\n",
    "*   Implementing hierarchical sampling (coarse and fine networks/sampling).\n",
    "*   Training on multiple views to enable novel view synthesis.\n",
    "*   Using a GPU for significantly faster training.\n",
    "*   Exploring libraries like `nerfstudio` for more advanced features and efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
