{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0dbb422",
   "metadata": {},
   "source": [
    "# Toy NeRF Implementation (CPU) - Multi-View\n",
    "\n",
    "This notebook demonstrates a minimal implementation of Neural Radiance Fields (NeRF) designed to run on a CPU.\n",
    "We will train on **multiple** synthetic images to enable basic novel view synthesis.\n",
    "**Note:** Training NeRF models is computationally intensive. This example uses a very small model, limited training iterations, and multiple views, so it will be **very slow** on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f24a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (2.2.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: imageio in /opt/conda/lib/python3.12/site-packages (2.37.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Installation (uncomment if needed)\n",
    "!pip install torch numpy matplotlib tqdm imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c3439a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import imageio.v3 as iio\n",
    "import os\n",
    "\n",
    "# Use GPU is available and CPU otherwise\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d358d7c9",
   "metadata": {},
   "source": [
    "## 2. Ray Generation\n",
    "\n",
    "We need functions to generate rays originating from the camera center and passing through each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2266ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(H, W, focal, c2w):\n",
    "    \"\"\"Generate rays for each pixel in the image.\"\"\"\n",
    "    # Create pixel coordinates\n",
    "    i, j = torch.meshgrid(torch.arange(W, dtype=torch.float32, device=device),\n",
    "                          torch.arange(H, dtype=torch.float32, device=device),\n",
    "                          indexing='xy')\n",
    "\n",
    "    # Convert pixel coordinates to camera coordinates\n",
    "    dirs = torch.stack([(i - W * 0.5) / focal,\n",
    "                        -(j - H * 0.5) / focal,\n",
    "                        -torch.ones_like(i)], dim=-1)\n",
    "\n",
    "    # Rotate directions from camera frame to world frame\n",
    "    rays_d = torch.sum(dirs[..., None, :] * c2w[:3, :3], -1)\n",
    "\n",
    "    # Translate camera frame origin to world frame origin\n",
    "    rays_o = c2w[:3, -1].expand(rays_d.shape)\n",
    "    return rays_o, rays_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae54685",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We'll use a standard synthetic dataset image (e.g., the lego bulldozer). We need the image itself and its corresponding camera pose.\n",
    "\n",
    "**Important:** The original download link is broken. You need to download the data manually:\n",
    "1. Go to the NeRF project page: [https://www.matthewtancik.com/nerf](https://www.matthewtancik.com/nerf)\n",
    "2. Find the 'Data' section and click the link for 'NeRF Synthetic Data'. This will likely lead to a Google Drive folder.\n",
    "3. Download the `nerf_synthetic.zip` file.\n",
    "4. Extract the zip file.\n",
    "5. Place the extracted `lego` folder inside the `nerf_synthetic_lego` directory in the same location as this notebook. You might need to create the `nerf_synthetic_lego` directory first. The final path should look like: `./nerf_synthetic_lego/lego/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b21a1e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Lego dataset...\n",
      "Error downloading dataset: 404 Client Error: Not Found for url: https://cseweb.ucsd.edu//~viscomp/projects/LF/papers/ECCV20/nerf/nerf_synthetic.zip\n",
      "Please download nerf_synthetic.zip manually from the NeRF website and extract the lego folder.\n",
      "Loading the first 20 views...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b928d55619ca4efe8e79c359f0d11991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resolution: 800x800, Focal length: 1111.11\n",
      "Loaded data shapes:\n",
      "Flattened target shape: torch.Size([12800000, 3])\n",
      "Flattened rays_o shape: torch.Size([12800000, 3])\n",
      "Flattened rays_d shape: torch.Size([12800000, 3])\n"
     ]
    }
   ],
   "source": [
    "# Define the number of views to load\n",
    "num_train_views = 20 # Start with 10 views\n",
    "\n",
    "# Download synthetic NeRF data (Lego bulldozer)\n",
    "if not os.path.exists('nerf_synthetic_lego.zip'):\n",
    "    print('Downloading Lego dataset...')\n",
    "    # Using a known URL for the lego dataset subset\n",
    "    # Note: This requires internet access and might take a moment.\n",
    "    # Consider manually downloading if this fails.\n",
    "    try:\n",
    "        import requests, zipfile, io\n",
    "        url = 'http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/nerf_synthetic.zip'\n",
    "        r = requests.get(url, stream=True)\n",
    "        r.raise_for_status() # Check if the download was successful\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall()\n",
    "        # Rename for clarity\n",
    "        os.rename('nerf_synthetic', 'nerf_synthetic_lego')\n",
    "        print('Download complete.')\n",
    "    except Exception as e:\n",
    "        print(f'Error downloading dataset: {e}')\n",
    "        print('Please download nerf_synthetic.zip manually from the NeRF website and extract the lego folder.')\n",
    "else:\n",
    "    print('Lego dataset already downloaded.')\n",
    "\n",
    "# Load multiple images and their poses\n",
    "data_dir = 'nerf_synthetic_lego/lego'\n",
    "split = 'train'\n",
    "\n",
    "all_target_pixels = []\n",
    "all_rays_o = []\n",
    "all_rays_d = []\n",
    "H, W, focal_length = None, None, None # Initialize these\n",
    "\n",
    "try:\n",
    "    # Load metadata\n",
    "    import json\n",
    "    with open(os.path.join(data_dir, f'transforms_{split}.json'), 'r') as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    print(f\"Loading the first {num_train_views} views...\")\n",
    "    for i in tqdm(range(num_train_views)):\n",
    "        frame = meta['frames'][i]\n",
    "        img_path = os.path.join(data_dir, frame['file_path'] + '.png')\n",
    "        target_img = iio.imread(img_path)\n",
    "        target_img = torch.tensor(target_img[..., :3] / 255.0, dtype=torch.float32).to(device) # Keep only RGB, normalize\n",
    "\n",
    "        pose = torch.tensor(frame['transform_matrix'], dtype=torch.float32).to(device)\n",
    "\n",
    "        if H is None: # Get intrinsics from the first image\n",
    "            H, W = target_img.shape[:2]\n",
    "            camera_angle_x = float(meta['camera_angle_x'])\n",
    "            focal_length = 0.5 * W / np.tan(0.5 * camera_angle_x)\n",
    "            print(f'Image resolution: {H}x{W}, Focal length: {focal_length:.2f}')\n",
    "\n",
    "        # Generate rays for the current view\n",
    "        rays_o, rays_d = get_rays(H, W, focal_length, pose)\n",
    "\n",
    "        # Append flattened data\n",
    "        all_target_pixels.append(target_img.reshape(-1, 3))\n",
    "        all_rays_o.append(rays_o.reshape(-1, 3))\n",
    "        all_rays_d.append(rays_d.reshape(-1, 3))\n",
    "\n",
    "    # Concatenate all data\n",
    "    target_img_flat = torch.cat(all_target_pixels, dim=0)\n",
    "    rays_o_flat = torch.cat(all_rays_o, dim=0)\n",
    "    rays_d_flat = torch.cat(all_rays_d, dim=0)\n",
    "\n",
    "    print(f'Loaded data shapes:')\n",
    "    print(f'Flattened target shape: {target_img_flat.shape}') # Should be (num_views*H*W, 3)\n",
    "    print(f'Flattened rays_o shape: {rays_o_flat.shape}') # Should be (num_views*H*W, 3)\n",
    "    print(f'Flattened rays_d shape: {rays_d_flat.shape}') # Should be (num_views*H*W, 3)\n",
    "\n",
    "    # Prepare data indices for batching\n",
    "    num_rays = rays_o_flat.shape[0]\n",
    "    data_loaded_successfully = True # Flag to indicate success\n",
    "\n",
    "except FileNotFoundError:\n",
    "     print(f'Error: Could not find dataset files in {data_dir}. Ensure the dataset was downloaded and extracted correctly.')\n",
    "     data_loaded_successfully = False\n",
    "except NameError as e:\n",
    "     print(f'Error: Make sure functions like get_rays are defined before this cell. {e}')\n",
    "     data_loaded_successfully = False\n",
    "except Exception as e:\n",
    "     print(f'An error occurred during data loading: {e}')\n",
    "     data_loaded_successfully = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de8fda",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5abbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecd6bf0e",
   "metadata": {},
   "source": [
    "## 3. Positional Encoding\n",
    "\n",
    "NeRF uses positional encoding to help the MLP represent high-frequency details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ad6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(x, num_freqs):\n",
    "    \"\"\"Apply positional encoding to input tensor.\"\"\"\n",
    "    freq_bands = 2.0**torch.arange(num_freqs, device=device) * torch.pi\n",
    "    scaled_x = x[..., None] * freq_bands\n",
    "    encoded = torch.cat([torch.sin(scaled_x), torch.cos(scaled_x)], dim=-1)\n",
    "    encoded = encoded.reshape(*x.shape[:-1], -1)\n",
    "    return torch.cat([x, encoded], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dfebde",
   "metadata": {},
   "source": [
    "## 4. NeRF Model (MLP)\n",
    "\n",
    "Define the simple Multi-Layer Perceptron (MLP) that forms the core of NeRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ea2d7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyNeRF(\n",
      "  (layer1): Linear(in_features=63, out_features=128, bias=True)\n",
      "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (layer3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (layer4): Linear(in_features=128, out_features=129, bias=True)\n",
      "  (layer5): Linear(in_features=155, out_features=64, bias=True)\n",
      "  (layer6): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TinyNeRF(nn.Module):\n",
    "    def __init__(self, pos_dim=3, dir_dim=3, num_pos_freqs=10, num_dir_freqs=4, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.pos_dim = pos_dim\n",
    "        self.dir_dim = dir_dim\n",
    "        self.num_pos_freqs = num_pos_freqs\n",
    "        self.num_dir_freqs = num_dir_freqs\n",
    "\n",
    "        self.encoded_pos_dim = pos_dim * (1 + 2 * num_pos_freqs)\n",
    "        self.encoded_dir_dim = dir_dim * (1 + 2 * num_dir_freqs)\n",
    "\n",
    "        self.layer1 = nn.Linear(self.encoded_pos_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer4 = nn.Linear(hidden_dim, hidden_dim + 1)\n",
    "\n",
    "        self.layer5 = nn.Linear(hidden_dim + self.encoded_dir_dim, hidden_dim // 2)\n",
    "        self.layer6 = nn.Linear(hidden_dim // 2, 3)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, pos, view_dir):\n",
    "        encoded_pos = positional_encoding(pos, self.num_pos_freqs)\n",
    "        encoded_dir = positional_encoding(view_dir, self.num_dir_freqs)\n",
    "\n",
    "        x = self.relu(self.layer1(encoded_pos))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        features = x[..., :-1]\n",
    "        sigma = self.relu(x[..., -1])\n",
    "\n",
    "        x = torch.cat([features, encoded_dir], dim=-1)\n",
    "        x = self.relu(self.layer5(x))\n",
    "        rgb = self.sigmoid(self.layer6(x))\n",
    "\n",
    "        return rgb, sigma\n",
    "\n",
    "# Instantiate the model\n",
    "model = TinyNeRF(hidden_dim=128).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b33ab",
   "metadata": {},
   "source": [
    "## 5. Volume Rendering\n",
    "\n",
    "Implement the volume rendering equation to composite colors and densities along each ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b648bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rays(model, rays_o, rays_d, near=2.0, far=6.0, num_samples=64):\n",
    "    t_vals = torch.linspace(near, far, num_samples, device=device)\n",
    "    points = rays_o[..., None, :] + rays_d[..., None, :] * t_vals[..., :, None]\n",
    "    view_dirs = rays_d[..., None, :].expand(-1, num_samples, -1)\n",
    "\n",
    "    points_flat = points.reshape(-1, 3)\n",
    "    view_dirs_flat = view_dirs.reshape(-1, 3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rgb_flat, sigma_flat = model(points_flat, view_dirs_flat)\n",
    "\n",
    "    rgb = rgb_flat.reshape(*points.shape[:-1], 3)\n",
    "    sigma = sigma_flat.reshape(*points.shape[:-1])\n",
    "\n",
    "    diffs = t_vals[1:] - t_vals[:-1]\n",
    "    dists_1d = torch.cat([diffs, torch.tensor([1e10], device=device)], dim=0)\n",
    "    dists = dists_1d.expand(sigma.shape)\n",
    "\n",
    "    alpha = 1.0 - torch.exp(-sigma * dists)\n",
    "    transmittance = torch.cumprod(torch.cat([torch.ones_like(alpha[:, :1]), 1.0 - alpha + 1e-10], dim=-1), dim=-1)[:, :-1]\n",
    "    weights = transmittance * alpha\n",
    "\n",
    "    rendered_color = torch.sum(weights[..., None] * rgb, dim=-2)\n",
    "\n",
    "    return rendered_color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c412e81",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "Set up the optimizer and run the training loop. We'll train on batches of rays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe5abdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 36000 iterations on 20 views...\n",
      "Total rays: 12800000, Batch size: 1024\n",
      "Saving checkpoints every 1000 iterations.\n",
      "This will be significantly slower than single-view training on CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5512c71b12c41b7b9505d6512e195ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/36000, Loss: 0.060177\n",
      "Iteration 200/36000, Loss: 0.054542\n",
      "Iteration 300/36000, Loss: 0.022856\n",
      "Iteration 400/36000, Loss: 0.018183\n",
      "Iteration 500/36000, Loss: 0.015953\n",
      "Iteration 600/36000, Loss: 0.015377\n",
      "Iteration 700/36000, Loss: 0.012164\n",
      "Iteration 800/36000, Loss: 0.013052\n",
      "Iteration 900/36000, Loss: 0.012195\n",
      "Iteration 1000/36000, Loss: 0.011097\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_1000iter_cuda.pth\n",
      "Iteration 1100/36000, Loss: 0.013812\n",
      "Iteration 1200/36000, Loss: 0.013076\n",
      "Iteration 1300/36000, Loss: 0.009606\n",
      "Iteration 1400/36000, Loss: 0.011507\n",
      "Iteration 1500/36000, Loss: 0.014682\n",
      "Iteration 1600/36000, Loss: 0.010448\n",
      "Iteration 1700/36000, Loss: 0.011613\n",
      "Iteration 1800/36000, Loss: 0.012131\n",
      "Iteration 1900/36000, Loss: 0.010465\n",
      "Iteration 2000/36000, Loss: 0.008908\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_2000iter_cuda.pth\n",
      "Iteration 2100/36000, Loss: 0.008819\n",
      "Iteration 2200/36000, Loss: 0.008674\n",
      "Iteration 2300/36000, Loss: 0.007220\n",
      "Iteration 2400/36000, Loss: 0.008218\n",
      "Iteration 2500/36000, Loss: 0.010143\n",
      "Iteration 2600/36000, Loss: 0.010685\n",
      "Iteration 2700/36000, Loss: 0.009219\n",
      "Iteration 2800/36000, Loss: 0.008456\n",
      "Iteration 2900/36000, Loss: 0.009645\n",
      "Iteration 3000/36000, Loss: 0.007916\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_3000iter_cuda.pth\n",
      "Iteration 3100/36000, Loss: 0.007639\n",
      "Iteration 3200/36000, Loss: 0.008603\n",
      "Iteration 3300/36000, Loss: 0.007790\n",
      "Iteration 3400/36000, Loss: 0.008801\n",
      "Iteration 3500/36000, Loss: 0.009601\n",
      "Iteration 3600/36000, Loss: 0.008804\n",
      "Iteration 3700/36000, Loss: 0.009064\n",
      "Iteration 3800/36000, Loss: 0.009246\n",
      "Iteration 3900/36000, Loss: 0.008665\n",
      "Iteration 4000/36000, Loss: 0.007738\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_4000iter_cuda.pth\n",
      "Iteration 4100/36000, Loss: 0.008052\n",
      "Iteration 4200/36000, Loss: 0.008378\n",
      "Iteration 4300/36000, Loss: 0.008177\n",
      "Iteration 4400/36000, Loss: 0.006646\n",
      "Iteration 4500/36000, Loss: 0.008279\n",
      "Iteration 4600/36000, Loss: 0.006596\n",
      "Iteration 4700/36000, Loss: 0.007622\n",
      "Iteration 4800/36000, Loss: 0.006308\n",
      "Iteration 4900/36000, Loss: 0.007119\n",
      "Iteration 5000/36000, Loss: 0.007240\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_5000iter_cuda.pth\n",
      "Iteration 5100/36000, Loss: 0.006776\n",
      "Iteration 5200/36000, Loss: 0.007447\n",
      "Iteration 5300/36000, Loss: 0.006576\n",
      "Iteration 5400/36000, Loss: 0.006715\n",
      "Iteration 5500/36000, Loss: 0.007936\n",
      "Iteration 5600/36000, Loss: 0.006858\n",
      "Iteration 5700/36000, Loss: 0.006281\n",
      "Iteration 5800/36000, Loss: 0.006688\n",
      "Iteration 5900/36000, Loss: 0.006823\n",
      "Iteration 6000/36000, Loss: 0.007433\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_6000iter_cuda.pth\n",
      "Iteration 6100/36000, Loss: 0.006866\n",
      "Iteration 6200/36000, Loss: 0.006777\n",
      "Iteration 6300/36000, Loss: 0.007791\n",
      "Iteration 6400/36000, Loss: 0.007959\n",
      "Iteration 6500/36000, Loss: 0.007169\n",
      "Iteration 6600/36000, Loss: 0.007242\n",
      "Iteration 6700/36000, Loss: 0.007540\n",
      "Iteration 6800/36000, Loss: 0.007955\n",
      "Iteration 6900/36000, Loss: 0.008062\n",
      "Iteration 7000/36000, Loss: 0.006375\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_7000iter_cuda.pth\n",
      "Iteration 7100/36000, Loss: 0.006865\n",
      "Iteration 7200/36000, Loss: 0.005711\n",
      "Iteration 7300/36000, Loss: 0.007703\n",
      "Iteration 7400/36000, Loss: 0.007570\n",
      "Iteration 7500/36000, Loss: 0.006006\n",
      "Iteration 7600/36000, Loss: 0.008165\n",
      "Iteration 7700/36000, Loss: 0.005954\n",
      "Iteration 7800/36000, Loss: 0.006445\n",
      "Iteration 7900/36000, Loss: 0.007127\n",
      "Iteration 8000/36000, Loss: 0.005897\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_8000iter_cuda.pth\n",
      "Iteration 8100/36000, Loss: 0.009011\n",
      "Iteration 8200/36000, Loss: 0.007186\n",
      "Iteration 8300/36000, Loss: 0.005828\n",
      "Iteration 8400/36000, Loss: 0.007664\n",
      "Iteration 8500/36000, Loss: 0.006346\n",
      "Iteration 8600/36000, Loss: 0.007662\n",
      "Iteration 8700/36000, Loss: 0.006883\n",
      "Iteration 8800/36000, Loss: 0.006092\n",
      "Iteration 8900/36000, Loss: 0.005755\n",
      "Iteration 9000/36000, Loss: 0.006645\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_9000iter_cuda.pth\n",
      "Iteration 9100/36000, Loss: 0.007114\n",
      "Iteration 9200/36000, Loss: 0.005862\n",
      "Iteration 9300/36000, Loss: 0.006310\n",
      "Iteration 9400/36000, Loss: 0.007281\n",
      "Iteration 9500/36000, Loss: 0.006379\n",
      "Iteration 9600/36000, Loss: 0.006850\n",
      "Iteration 9700/36000, Loss: 0.007258\n",
      "Iteration 9800/36000, Loss: 0.006172\n",
      "Iteration 9900/36000, Loss: 0.007260\n",
      "Iteration 10000/36000, Loss: 0.006180\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_10000iter_cuda.pth\n",
      "Iteration 10100/36000, Loss: 0.006778\n",
      "Iteration 10200/36000, Loss: 0.006719\n",
      "Iteration 10300/36000, Loss: 0.006930\n",
      "Iteration 10400/36000, Loss: 0.005599\n",
      "Iteration 10500/36000, Loss: 0.004837\n",
      "Iteration 10600/36000, Loss: 0.005417\n",
      "Iteration 10700/36000, Loss: 0.005150\n",
      "Iteration 10800/36000, Loss: 0.006804\n",
      "Iteration 10900/36000, Loss: 0.005619\n",
      "Iteration 11000/36000, Loss: 0.006193\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_11000iter_cuda.pth\n",
      "Iteration 11100/36000, Loss: 0.005161\n",
      "Iteration 11200/36000, Loss: 0.006470\n",
      "Iteration 11300/36000, Loss: 0.005932\n",
      "Iteration 11400/36000, Loss: 0.006205\n",
      "Iteration 11500/36000, Loss: 0.005895\n",
      "Iteration 11600/36000, Loss: 0.006388\n",
      "Iteration 11700/36000, Loss: 0.006083\n",
      "Iteration 11800/36000, Loss: 0.006382\n",
      "Iteration 11900/36000, Loss: 0.005590\n",
      "Iteration 12000/36000, Loss: 0.005995\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_12000iter_cuda.pth\n",
      "Iteration 12100/36000, Loss: 0.005944\n",
      "Iteration 12200/36000, Loss: 0.005245\n",
      "Iteration 12300/36000, Loss: 0.006577\n",
      "Iteration 12400/36000, Loss: 0.004772\n",
      "Iteration 12500/36000, Loss: 0.005531\n",
      "Iteration 12600/36000, Loss: 0.005964\n",
      "Iteration 12700/36000, Loss: 0.007261\n",
      "Iteration 12800/36000, Loss: 0.007485\n",
      "Iteration 12900/36000, Loss: 0.006210\n",
      "Iteration 13000/36000, Loss: 0.006368\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_13000iter_cuda.pth\n",
      "Iteration 13100/36000, Loss: 0.005896\n",
      "Iteration 13200/36000, Loss: 0.006940\n",
      "Iteration 13300/36000, Loss: 0.005429\n",
      "Iteration 13400/36000, Loss: 0.008507\n",
      "Iteration 13500/36000, Loss: 0.006006\n",
      "Iteration 13600/36000, Loss: 0.005937\n",
      "Iteration 13700/36000, Loss: 0.006108\n",
      "Iteration 13800/36000, Loss: 0.006606\n",
      "Iteration 13900/36000, Loss: 0.006780\n",
      "Iteration 14000/36000, Loss: 0.005661\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_14000iter_cuda.pth\n",
      "Iteration 14100/36000, Loss: 0.005431\n",
      "Iteration 14200/36000, Loss: 0.006265\n",
      "Iteration 14300/36000, Loss: 0.005020\n",
      "Iteration 14400/36000, Loss: 0.006030\n",
      "Iteration 14500/36000, Loss: 0.004156\n",
      "Iteration 14600/36000, Loss: 0.005852\n",
      "Iteration 14700/36000, Loss: 0.006220\n",
      "Iteration 14800/36000, Loss: 0.005452\n",
      "Iteration 14900/36000, Loss: 0.005266\n",
      "Iteration 15000/36000, Loss: 0.006113\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_15000iter_cuda.pth\n",
      "Iteration 15100/36000, Loss: 0.004783\n",
      "Iteration 15200/36000, Loss: 0.005709\n",
      "Iteration 15300/36000, Loss: 0.005463\n",
      "Iteration 15400/36000, Loss: 0.006742\n",
      "Iteration 15500/36000, Loss: 0.005884\n",
      "Iteration 15600/36000, Loss: 0.005204\n",
      "Iteration 15700/36000, Loss: 0.004564\n",
      "Iteration 15800/36000, Loss: 0.005227\n",
      "Iteration 15900/36000, Loss: 0.006105\n",
      "Iteration 16000/36000, Loss: 0.006045\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_16000iter_cuda.pth\n",
      "Iteration 16100/36000, Loss: 0.005257\n",
      "Iteration 16200/36000, Loss: 0.004802\n",
      "Iteration 16300/36000, Loss: 0.005937\n",
      "Iteration 16400/36000, Loss: 0.005488\n",
      "Iteration 16500/36000, Loss: 0.005875\n",
      "Iteration 16600/36000, Loss: 0.005431\n",
      "Iteration 16700/36000, Loss: 0.005618\n",
      "Iteration 16800/36000, Loss: 0.005789\n",
      "Iteration 16900/36000, Loss: 0.005919\n",
      "Iteration 17000/36000, Loss: 0.004534\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_17000iter_cuda.pth\n",
      "Iteration 17100/36000, Loss: 0.004751\n",
      "Iteration 17200/36000, Loss: 0.004614\n",
      "Iteration 17300/36000, Loss: 0.005661\n",
      "Iteration 17400/36000, Loss: 0.005361\n",
      "Iteration 17500/36000, Loss: 0.005981\n",
      "Iteration 17600/36000, Loss: 0.005158\n",
      "Iteration 17700/36000, Loss: 0.006967\n",
      "Iteration 17800/36000, Loss: 0.005807\n",
      "Iteration 17900/36000, Loss: 0.004971\n",
      "Iteration 18000/36000, Loss: 0.005885\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_18000iter_cuda.pth\n",
      "Iteration 18100/36000, Loss: 0.006235\n",
      "Iteration 18200/36000, Loss: 0.006383\n",
      "Iteration 18300/36000, Loss: 0.006167\n",
      "Iteration 18400/36000, Loss: 0.004742\n",
      "Iteration 18500/36000, Loss: 0.005570\n",
      "Iteration 18600/36000, Loss: 0.004681\n",
      "Iteration 18700/36000, Loss: 0.005243\n",
      "Iteration 18800/36000, Loss: 0.005266\n",
      "Iteration 18900/36000, Loss: 0.005778\n",
      "Iteration 19000/36000, Loss: 0.004664\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_19000iter_cuda.pth\n",
      "Iteration 19100/36000, Loss: 0.004785\n",
      "Iteration 19200/36000, Loss: 0.005394\n",
      "Iteration 19300/36000, Loss: 0.005881\n",
      "Iteration 19400/36000, Loss: 0.006151\n",
      "Iteration 19500/36000, Loss: 0.005324\n",
      "Iteration 19600/36000, Loss: 0.005143\n",
      "Iteration 19700/36000, Loss: 0.005360\n",
      "Iteration 19800/36000, Loss: 0.005302\n",
      "Iteration 19900/36000, Loss: 0.005481\n",
      "Iteration 20000/36000, Loss: 0.005284\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_20000iter_cuda.pth\n",
      "Iteration 20100/36000, Loss: 0.006262\n",
      "Iteration 20200/36000, Loss: 0.005469\n",
      "Iteration 20300/36000, Loss: 0.005810\n",
      "Iteration 20400/36000, Loss: 0.005183\n",
      "Iteration 20500/36000, Loss: 0.004973\n",
      "Iteration 20600/36000, Loss: 0.006000\n",
      "Iteration 20700/36000, Loss: 0.006288\n",
      "Iteration 20800/36000, Loss: 0.005517\n",
      "Iteration 20900/36000, Loss: 0.006272\n",
      "Iteration 21000/36000, Loss: 0.005434\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_21000iter_cuda.pth\n",
      "Iteration 21100/36000, Loss: 0.005806\n",
      "Iteration 21200/36000, Loss: 0.004710\n",
      "Iteration 21300/36000, Loss: 0.005367\n",
      "Iteration 21400/36000, Loss: 0.005398\n",
      "Iteration 21500/36000, Loss: 0.004956\n",
      "Iteration 21600/36000, Loss: 0.004548\n",
      "Iteration 21700/36000, Loss: 0.005913\n",
      "Iteration 21800/36000, Loss: 0.007021\n",
      "Iteration 21900/36000, Loss: 0.005868\n",
      "Iteration 22000/36000, Loss: 0.004790\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_22000iter_cuda.pth\n",
      "Iteration 22100/36000, Loss: 0.004584\n",
      "Iteration 22200/36000, Loss: 0.005278\n",
      "Iteration 22300/36000, Loss: 0.005175\n",
      "Iteration 22400/36000, Loss: 0.005272\n",
      "Iteration 22500/36000, Loss: 0.005118\n",
      "Iteration 22600/36000, Loss: 0.005539\n",
      "Iteration 22700/36000, Loss: 0.005519\n",
      "Iteration 22800/36000, Loss: 0.006021\n",
      "Iteration 22900/36000, Loss: 0.006043\n",
      "Iteration 23000/36000, Loss: 0.006036\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_23000iter_cuda.pth\n",
      "Iteration 23100/36000, Loss: 0.005230\n",
      "Iteration 23200/36000, Loss: 0.004976\n",
      "Iteration 23300/36000, Loss: 0.005464\n",
      "Iteration 23400/36000, Loss: 0.004898\n",
      "Iteration 23500/36000, Loss: 0.005224\n",
      "Iteration 23600/36000, Loss: 0.004819\n",
      "Iteration 23700/36000, Loss: 0.005038\n",
      "Iteration 23800/36000, Loss: 0.005011\n",
      "Iteration 23900/36000, Loss: 0.005613\n",
      "Iteration 24000/36000, Loss: 0.004736\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_24000iter_cuda.pth\n",
      "Iteration 24100/36000, Loss: 0.004681\n",
      "Iteration 24200/36000, Loss: 0.004707\n",
      "Iteration 24300/36000, Loss: 0.006218\n",
      "Iteration 24400/36000, Loss: 0.005609\n",
      "Iteration 24500/36000, Loss: 0.005140\n",
      "Iteration 24600/36000, Loss: 0.004513\n",
      "Iteration 24700/36000, Loss: 0.005790\n",
      "Iteration 24800/36000, Loss: 0.004844\n",
      "Iteration 24900/36000, Loss: 0.004947\n",
      "Iteration 25000/36000, Loss: 0.006171\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_25000iter_cuda.pth\n",
      "Iteration 25100/36000, Loss: 0.007290\n",
      "Iteration 25200/36000, Loss: 0.004874\n",
      "Iteration 25300/36000, Loss: 0.005441\n",
      "Iteration 25400/36000, Loss: 0.005738\n",
      "Iteration 25500/36000, Loss: 0.003196\n",
      "Iteration 25600/36000, Loss: 0.004853\n",
      "Iteration 25700/36000, Loss: 0.005449\n",
      "Iteration 25800/36000, Loss: 0.005820\n",
      "Iteration 25900/36000, Loss: 0.005751\n",
      "Iteration 26000/36000, Loss: 0.003904\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_26000iter_cuda.pth\n",
      "Iteration 26100/36000, Loss: 0.004939\n",
      "Iteration 26200/36000, Loss: 0.005272\n",
      "Iteration 26300/36000, Loss: 0.005007\n",
      "Iteration 26400/36000, Loss: 0.004831\n",
      "Iteration 26500/36000, Loss: 0.005590\n",
      "Iteration 26600/36000, Loss: 0.004673\n",
      "Iteration 26700/36000, Loss: 0.004826\n",
      "Iteration 26800/36000, Loss: 0.005836\n",
      "Iteration 26900/36000, Loss: 0.005220\n",
      "Iteration 27000/36000, Loss: 0.003791\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_27000iter_cuda.pth\n",
      "Iteration 27100/36000, Loss: 0.004643\n",
      "Iteration 27200/36000, Loss: 0.004482\n",
      "Iteration 27300/36000, Loss: 0.006403\n",
      "Iteration 27400/36000, Loss: 0.005587\n",
      "Iteration 27500/36000, Loss: 0.005643\n",
      "Iteration 27600/36000, Loss: 0.005674\n",
      "Iteration 27700/36000, Loss: 0.005798\n",
      "Iteration 27800/36000, Loss: 0.005399\n",
      "Iteration 27900/36000, Loss: 0.004685\n",
      "Iteration 28000/36000, Loss: 0.005188\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_28000iter_cuda.pth\n",
      "Iteration 28100/36000, Loss: 0.004571\n",
      "Iteration 28200/36000, Loss: 0.005073\n",
      "Iteration 28300/36000, Loss: 0.004830\n",
      "Iteration 28400/36000, Loss: 0.005366\n",
      "Iteration 28500/36000, Loss: 0.004212\n",
      "Iteration 28600/36000, Loss: 0.005244\n",
      "Iteration 28700/36000, Loss: 0.004336\n",
      "Iteration 28800/36000, Loss: 0.004088\n",
      "Iteration 28900/36000, Loss: 0.005189\n",
      "Iteration 29000/36000, Loss: 0.004196\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_29000iter_cuda.pth\n",
      "Iteration 29100/36000, Loss: 0.004285\n",
      "Iteration 29200/36000, Loss: 0.004113\n",
      "Iteration 29300/36000, Loss: 0.006262\n",
      "Iteration 29400/36000, Loss: 0.004386\n",
      "Iteration 29500/36000, Loss: 0.004587\n",
      "Iteration 29600/36000, Loss: 0.005168\n",
      "Iteration 29700/36000, Loss: 0.004413\n",
      "Iteration 29800/36000, Loss: 0.005322\n",
      "Iteration 29900/36000, Loss: 0.005370\n",
      "Iteration 30000/36000, Loss: 0.003779\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_30000iter_cuda.pth\n",
      "Iteration 30100/36000, Loss: 0.004903\n",
      "Iteration 30200/36000, Loss: 0.004442\n",
      "Iteration 30300/36000, Loss: 0.005459\n",
      "Iteration 30400/36000, Loss: 0.004514\n",
      "Iteration 30500/36000, Loss: 0.004578\n",
      "Iteration 30600/36000, Loss: 0.005071\n",
      "Iteration 30700/36000, Loss: 0.005027\n",
      "Iteration 30800/36000, Loss: 0.005065\n",
      "Iteration 30900/36000, Loss: 0.004687\n",
      "Iteration 31000/36000, Loss: 0.005779\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_31000iter_cuda.pth\n",
      "Iteration 31100/36000, Loss: 0.004754\n",
      "Iteration 31200/36000, Loss: 0.004534\n",
      "Iteration 31300/36000, Loss: 0.004497\n",
      "Iteration 31400/36000, Loss: 0.004159\n",
      "Iteration 31500/36000, Loss: 0.004825\n",
      "Iteration 31600/36000, Loss: 0.005944\n",
      "Iteration 31700/36000, Loss: 0.005173\n",
      "Iteration 31800/36000, Loss: 0.004911\n",
      "Iteration 31900/36000, Loss: 0.005300\n",
      "Iteration 32000/36000, Loss: 0.005510\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_32000iter_cuda.pth\n",
      "Iteration 32100/36000, Loss: 0.004257\n",
      "Iteration 32200/36000, Loss: 0.005290\n",
      "Iteration 32300/36000, Loss: 0.004840\n",
      "Iteration 32400/36000, Loss: 0.004212\n",
      "Iteration 32500/36000, Loss: 0.005045\n",
      "Iteration 32600/36000, Loss: 0.006216\n",
      "Iteration 32700/36000, Loss: 0.005341\n",
      "Iteration 32800/36000, Loss: 0.006065\n",
      "Iteration 32900/36000, Loss: 0.004824\n",
      "Iteration 33000/36000, Loss: 0.005081\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_33000iter_cuda.pth\n",
      "Iteration 33100/36000, Loss: 0.005591\n",
      "Iteration 33200/36000, Loss: 0.005186\n",
      "Iteration 33300/36000, Loss: 0.005356\n",
      "Iteration 33400/36000, Loss: 0.004438\n",
      "Iteration 33500/36000, Loss: 0.005361\n",
      "Iteration 33600/36000, Loss: 0.004624\n",
      "Iteration 33700/36000, Loss: 0.004754\n",
      "Iteration 33800/36000, Loss: 0.004236\n",
      "Iteration 33900/36000, Loss: 0.004012\n",
      "Iteration 34000/36000, Loss: 0.004872\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_34000iter_cuda.pth\n",
      "Iteration 34100/36000, Loss: 0.005224\n",
      "Iteration 34200/36000, Loss: 0.003807\n",
      "Iteration 34300/36000, Loss: 0.004698\n",
      "Iteration 34400/36000, Loss: 0.004573\n",
      "Iteration 34500/36000, Loss: 0.004950\n",
      "Iteration 34600/36000, Loss: 0.004339\n",
      "Iteration 34700/36000, Loss: 0.005925\n",
      "Iteration 34800/36000, Loss: 0.004579\n",
      "Iteration 34900/36000, Loss: 0.004071\n",
      "Iteration 35000/36000, Loss: 0.005877\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_35000iter_cuda.pth\n",
      "Iteration 35100/36000, Loss: 0.005486\n",
      "Iteration 35200/36000, Loss: 0.004886\n",
      "Iteration 35300/36000, Loss: 0.004070\n",
      "Iteration 35400/36000, Loss: 0.005628\n",
      "Iteration 35500/36000, Loss: 0.004583\n",
      "Iteration 35600/36000, Loss: 0.004894\n",
      "Iteration 35700/36000, Loss: 0.004619\n",
      "Iteration 35800/36000, Loss: 0.004385\n",
      "Iteration 35900/36000, Loss: 0.005069\n",
      "Iteration 36000/36000, Loss: 0.005572\n",
      "Checkpoint saved to tiny_nerf_lego_multi_20views_36000iter_cuda.pth\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "learning_rate = 5e-4\n",
    "num_iterations = 36000\n",
    "batch_size = 1024\n",
    "display_rate = 100\n",
    "save_rate = 1000 # Save checkpoint every 1000 iterations\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Check if data loaded successfully before starting training\n",
    "if 'data_loaded_successfully' in locals() and data_loaded_successfully:\n",
    "    print(f'Starting training for {num_iterations} iterations on {num_train_views} views...')\n",
    "    print(f'Total rays: {num_rays}, Batch size: {batch_size}')\n",
    "    print(f'Saving checkpoints every {save_rate} iterations.')\n",
    "    print('This will be significantly slower than single-view training on CPU.')\n",
    "\n",
    "    model.train()\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        indices = torch.randint(0, num_rays, (batch_size,))\n",
    "        batch_rays_o = rays_o_flat[indices].to(device)\n",
    "        batch_rays_d = rays_d_flat[indices].to(device)\n",
    "        batch_target_pixels = target_img_flat[indices].to(device)\n",
    "\n",
    "        t_vals = torch.linspace(2.0, 6.0, 64, device=device)\n",
    "        t_rand = torch.rand(batch_size, 64, device=device) * (t_vals[1] - t_vals[0])\n",
    "        t_vals_noisy = t_vals + t_rand\n",
    "\n",
    "        points = batch_rays_o[..., None, :] + batch_rays_d[..., None, :] * t_vals_noisy[..., :, None]\n",
    "        view_dirs = batch_rays_d[..., None, :].expand(-1, 64, -1)\n",
    "\n",
    "        points_flat = points.reshape(-1, 3)\n",
    "        view_dirs_flat = view_dirs.reshape(-1, 3)\n",
    "\n",
    "        rgb_flat, sigma_flat = model(points_flat, view_dirs_flat)\n",
    "\n",
    "        rgb = rgb_flat.reshape(batch_size, 64, 3)\n",
    "        sigma = sigma_flat.reshape(batch_size, 64)\n",
    "\n",
    "        dists = torch.cat([t_vals_noisy[:, 1:] - t_vals_noisy[:, :-1], torch.tensor([1e10], device=device).expand(batch_size, 1)], dim=-1)\n",
    "        alpha = 1.0 - torch.exp(-sigma * dists)\n",
    "        transmittance = torch.cumprod(torch.cat([torch.ones_like(alpha[:, :1]), 1.0 - alpha + 1e-10], dim=-1), dim=-1)[:, :-1]\n",
    "        weights = transmittance * alpha\n",
    "        rendered_color = torch.sum(weights[..., None] * rgb, dim=-2)\n",
    "\n",
    "        loss = mse_loss(rendered_color, batch_target_pixels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % display_rate == 0:\n",
    "            tqdm.write(f'Iteration {i+1}/{num_iterations}, Loss: {loss.item():.6f}')\n",
    "\n",
    "        # --- Checkpoint Saving ---\n",
    "        if (i + 1) % save_rate == 0:\n",
    "            save_num_views_ckpt = num_train_views if 'num_train_views' in locals() else 'multi'\n",
    "            checkpoint_path = f'tiny_nerf_lego_multi_{save_num_views_ckpt}views_{i+1}iter_{device}.pth'\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            tqdm.write(f'Checkpoint saved to {checkpoint_path}')\n",
    "        # --- End Checkpoint Saving ---\n",
    "\n",
    "    print('Training finished.')\n",
    "else:\n",
    "    print('Skipping training because data loading failed in the previous step.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f1453",
   "metadata": {},
   "source": [
    "## 7. Visualization\n",
    "\n",
    "Render the full image using the trained model and compare it to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "701a8be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load test image: 'NoneType' object is not subscriptable\n",
      "Skipping test image rendering due to loading error.\n"
     ]
    }
   ],
   "source": [
    "# Check if training ran before attempting visualization\n",
    "if 'data_loaded_successfully' in locals() and data_loaded_successfully:\n",
    "    model.eval()\n",
    "\n",
    "    # --- Load a test view for evaluation ---\n",
    "    test_split = 'test'\n",
    "    test_img_idx = 0\n",
    "    test_target_img, test_pose = None, None\n",
    "    try:\n",
    "        with open(os.path.join(data_dir, f'transforms_{test_split}.json'), 'r') as f:\n",
    "            test_meta = json.load(f)\n",
    "        test_frame = test_meta['frames'][test_img_idx]\n",
    "        test_img_path = os.path.join(data_dir, test_frame['file_path'] + '.png')\n",
    "        if H is None or W is None or focal_length is None:\n",
    "             raise ValueError(\"Image dimensions or focal length not loaded correctly.\")\n",
    "        test_target_img = torch.tensor(test_target_img[..., :3] / 255.0, dtype=torch.float32).to(device)\n",
    "        test_pose = torch.tensor(test_frame['transform_matrix'], dtype=torch.float32).to(device)\n",
    "        print(f\"Loaded test image {test_img_idx} for evaluation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load test image: {e}\")\n",
    "    # --- End loading test view ---\n",
    "\n",
    "    if test_target_img is not None and test_pose is not None:\n",
    "        test_rays_o, test_rays_d = get_rays(H, W, focal_length, test_pose)\n",
    "        test_rays_o_flat = test_rays_o.reshape(-1, 3)\n",
    "        test_rays_d_flat = test_rays_d.reshape(-1, 3)\n",
    "        test_num_rays = test_rays_o_flat.shape[0]\n",
    "\n",
    "        rendered_image_flat = []\n",
    "        render_batch_size = 2048\n",
    "\n",
    "        print(f'Rendering test image (using {render_batch_size} rays per batch)...')\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, test_num_rays, render_batch_size)):\n",
    "                batch_rays_o = test_rays_o_flat[i:i+render_batch_size].to(device)\n",
    "                batch_rays_d = test_rays_d_flat[i:i+render_batch_size].to(device)\n",
    "                rendered_batch = render_rays(model, batch_rays_o, batch_rays_d, near=2.0, far=6.0, num_samples=64)\n",
    "                rendered_image_flat.append(rendered_batch)\n",
    "\n",
    "        rendered_image = torch.cat(rendered_image_flat, dim=0).reshape(H, W, 3)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        axes[0].imshow(test_target_img.cpu().numpy())\n",
    "        axes[0].set_title(f'Test Image ({test_img_idx})')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(rendered_image.cpu().numpy())\n",
    "        axes[1].set_title(f'Rendered Test Image (After {num_iterations} iterations)')\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        mse = torch.mean((rendered_image - test_target_img)**2)\n",
    "        psnr = -10.0 * torch.log10(mse)\n",
    "        print(f'Test Image PSNR: {psnr.item():.2f} dB')\n",
    "    else:\n",
    "        print(\"Skipping test image rendering due to loading error.\")\n",
    "else:\n",
    "    print('Skipping visualization because data loading or training failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27293c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model weights saved to tiny_nerf_lego_multi_20views_36000iterations_final_cuda.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the final trained model weights\n",
    "if 'data_loaded_successfully' in locals() and data_loaded_successfully:\n",
    "    save_num_views = num_train_views if 'num_train_views' in locals() else 'multi'\n",
    "    model_save_path = f'tiny_nerf_lego_multi_{save_num_views}views_{num_iterations}iterations_final_{device}.pth' # Added _final\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f'Final model weights saved to {model_save_path}')\n",
    "else:\n",
    "    print('Skipping model saving because training did not complete successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a3aa1e",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook provided a minimal NeRF implementation focused on overfitting a single image on the CPU.\n",
    "Key takeaways:\n",
    "*   NeRF combines positional encoding, an MLP, and volume rendering.\n",
    "*   Training even a small model on a single image is slow on CPU.\n",
    "*   The quality depends heavily on model size, number of samples per ray, and training iterations.\n",
    "\n",
    "Further exploration could involve:\n",
    "*   Implementing hierarchical sampling (coarse and fine networks/sampling).\n",
    "*   Training on multiple views to enable novel view synthesis.\n",
    "*   Using a GPU for significantly faster training.\n",
    "*   Exploring libraries like `nerfstudio` for more advanced features and efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
