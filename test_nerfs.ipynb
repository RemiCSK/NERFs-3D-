{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0dbb422",
   "metadata": {},
   "source": [
    "# Toy NeRF Implementation (CPU) - Multi-View\n",
    "\n",
    "This notebook demonstrates a minimal implementation of Neural Radiance Fields (NeRF) designed to run on a CPU.\n",
    "We will train on **multiple** synthetic images to enable basic novel view synthesis.\n",
    "**Note:** Training NeRF models is computationally intensive. This example uses a very small model, limited training iterations, and multiple views, so it will be **very slow** on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88f24a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\rems9\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\rems9\\anaconda3\\lib\\site-packages (1.24.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rems9\\anaconda3\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rems9\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: imageio in c:\\users\\rems9\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from matplotlib) (5.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\rems9\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rems9\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rems9\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Installation (uncomment if needed)\n",
    "!pip install torch numpy matplotlib tqdm imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3439a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import imageio.v3 as iio\n",
    "import os\n",
    "\n",
    "# Use GPU is available and CPU otherwise\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae54685",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We'll use a standard synthetic dataset image (e.g., the lego bulldozer). We need the image itself and its corresponding camera pose.\n",
    "\n",
    "**Important:** The original download link is broken. You need to download the data manually:\n",
    "1. Go to the NeRF project page: [https://www.matthewtancik.com/nerf](https://www.matthewtancik.com/nerf)\n",
    "2. Find the 'Data' section and click the link for 'NeRF Synthetic Data'. This will likely lead to a Google Drive folder.\n",
    "3. Download the `nerf_synthetic.zip` file.\n",
    "4. Extract the zip file.\n",
    "5. Place the extracted `lego` folder inside the `nerf_synthetic_lego` directory in the same location as this notebook. You might need to create the `nerf_synthetic_lego` directory first. The final path should look like: `./nerf_synthetic_lego/lego/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b21a1e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Lego dataset...\n",
      "Error downloading dataset: 404 Client Error: Not Found for url: https://cseweb.ucsd.edu//~viscomp/projects/LF/papers/ECCV20/nerf/nerf_synthetic.zip\n",
      "Please download nerf_synthetic.zip manually from the NeRF website and extract the lego folder.\n",
      "Loading the first 20 views...\n",
      "Error downloading dataset: 404 Client Error: Not Found for url: https://cseweb.ucsd.edu//~viscomp/projects/LF/papers/ECCV20/nerf/nerf_synthetic.zip\n",
      "Please download nerf_synthetic.zip manually from the NeRF website and extract the lego folder.\n",
      "Loading the first 20 views...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c258a5eab33142f5b504764a52a8fb29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resolution: 800x800, Focal length: 1111.11\n",
      "Loaded data shapes:\n",
      "Flattened target shape: torch.Size([12800000, 3])\n",
      "Flattened rays_o shape: torch.Size([12800000, 3])\n",
      "Flattened rays_d shape: torch.Size([12800000, 3])\n",
      "Loaded data shapes:\n",
      "Flattened target shape: torch.Size([12800000, 3])\n",
      "Flattened rays_o shape: torch.Size([12800000, 3])\n",
      "Flattened rays_d shape: torch.Size([12800000, 3])\n"
     ]
    }
   ],
   "source": [
    "# Define the number of views to load\n",
    "num_train_views = 20 # Start with 10 views\n",
    "\n",
    "# Download synthetic NeRF data (Lego bulldozer)\n",
    "if not os.path.exists('nerf_synthetic_lego.zip'):\n",
    "    print('Downloading Lego dataset...')\n",
    "    # Using a known URL for the lego dataset subset\n",
    "    # Note: This requires internet access and might take a moment.\n",
    "    # Consider manually downloading if this fails.\n",
    "    try:\n",
    "        import requests, zipfile, io\n",
    "        url = 'http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/nerf_synthetic.zip'\n",
    "        r = requests.get(url, stream=True)\n",
    "        r.raise_for_status() # Check if the download was successful\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall()\n",
    "        # Rename for clarity\n",
    "        os.rename('nerf_synthetic', 'nerf_synthetic_lego')\n",
    "        print('Download complete.')\n",
    "    except Exception as e:\n",
    "        print(f'Error downloading dataset: {e}')\n",
    "        print('Please download nerf_synthetic.zip manually from the NeRF website and extract the lego folder.')\n",
    "else:\n",
    "    print('Lego dataset already downloaded.')\n",
    "\n",
    "# Load multiple images and their poses\n",
    "data_dir = 'nerf_synthetic_lego/lego'\n",
    "split = 'train'\n",
    "\n",
    "all_target_pixels = []\n",
    "all_rays_o = []\n",
    "all_rays_d = []\n",
    "H, W, focal_length = None, None, None # Initialize these\n",
    "\n",
    "try:\n",
    "    # Load metadata\n",
    "    import json\n",
    "    with open(os.path.join(data_dir, f'transforms_{split}.json'), 'r') as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    print(f\"Loading the first {num_train_views} views...\")\n",
    "    for i in tqdm(range(num_train_views)):\n",
    "        frame = meta['frames'][i]\n",
    "        img_path = os.path.join(data_dir, frame['file_path'] + '.png')\n",
    "        target_img = iio.imread(img_path)\n",
    "        target_img = torch.tensor(target_img[..., :3] / 255.0, dtype=torch.float32).to(device) # Keep only RGB, normalize\n",
    "\n",
    "        pose = torch.tensor(frame['transform_matrix'], dtype=torch.float32).to(device)\n",
    "\n",
    "        if H is None: # Get intrinsics from the first image\n",
    "            H, W = target_img.shape[:2]\n",
    "            camera_angle_x = float(meta['camera_angle_x'])\n",
    "            focal_length = 0.5 * W / np.tan(0.5 * camera_angle_x)\n",
    "            print(f'Image resolution: {H}x{W}, Focal length: {focal_length:.2f}')\n",
    "\n",
    "        # Generate rays for the current view\n",
    "        rays_o, rays_d = get_rays(H, W, focal_length, pose)\n",
    "\n",
    "        # Append flattened data\n",
    "        all_target_pixels.append(target_img.reshape(-1, 3))\n",
    "        all_rays_o.append(rays_o.reshape(-1, 3))\n",
    "        all_rays_d.append(rays_d.reshape(-1, 3))\n",
    "\n",
    "    # Concatenate all data\n",
    "    target_img_flat = torch.cat(all_target_pixels, dim=0)\n",
    "    rays_o_flat = torch.cat(all_rays_o, dim=0)\n",
    "    rays_d_flat = torch.cat(all_rays_d, dim=0)\n",
    "\n",
    "    print(f'Loaded data shapes:')\n",
    "    print(f'Flattened target shape: {target_img_flat.shape}') # Should be (num_views*H*W, 3)\n",
    "    print(f'Flattened rays_o shape: {rays_o_flat.shape}') # Should be (num_views*H*W, 3)\n",
    "    print(f'Flattened rays_d shape: {rays_d_flat.shape}') # Should be (num_views*H*W, 3)\n",
    "\n",
    "    # Prepare data indices for batching\n",
    "    num_rays = rays_o_flat.shape[0]\n",
    "    data_loaded_successfully = True # Flag to indicate success\n",
    "\n",
    "except FileNotFoundError:\n",
    "     print(f'Error: Could not find dataset files in {data_dir}. Ensure the dataset was downloaded and extracted correctly.')\n",
    "     data_loaded_successfully = False\n",
    "except NameError as e:\n",
    "     print(f'Error: Make sure functions like get_rays are defined before this cell. {e}')\n",
    "     data_loaded_successfully = False\n",
    "except Exception as e:\n",
    "     print(f'An error occurred during data loading: {e}')\n",
    "     data_loaded_successfully = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de8fda",
   "metadata": {},
   "source": [
    "## 2. Ray Generation\n",
    "\n",
    "We need functions to generate rays originating from the camera center and passing through each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f5abbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(H, W, focal, c2w):\n",
    "    \"\"\"Generate rays for each pixel in the image.\"\"\"\n",
    "    # Create pixel coordinates\n",
    "    i, j = torch.meshgrid(torch.arange(W, dtype=torch.float32, device=device),\n",
    "                          torch.arange(H, dtype=torch.float32, device=device),\n",
    "                          indexing='xy')\n",
    "\n",
    "    # Convert pixel coordinates to camera coordinates\n",
    "    dirs = torch.stack([(i - W * 0.5) / focal,\n",
    "                        -(j - H * 0.5) / focal,\n",
    "                        -torch.ones_like(i)], dim=-1)\n",
    "\n",
    "    # Rotate directions from camera frame to world frame\n",
    "    rays_d = torch.sum(dirs[..., None, :] * c2w[:3, :3], -1)\n",
    "\n",
    "    # Translate camera frame origin to world frame origin\n",
    "    rays_o = c2w[:3, -1].expand(rays_d.shape)\n",
    "    return rays_o, rays_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6bf0e",
   "metadata": {},
   "source": [
    "## 3. Positional Encoding\n",
    "\n",
    "NeRF uses positional encoding to help the MLP represent high-frequency details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67ad6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(x, num_freqs):\n",
    "    \"\"\"Apply positional encoding to input tensor.\"\"\"\n",
    "    freq_bands = 2.0**torch.arange(num_freqs, device=device) * torch.pi\n",
    "    scaled_x = x[..., None] * freq_bands\n",
    "    encoded = torch.cat([torch.sin(scaled_x), torch.cos(scaled_x)], dim=-1)\n",
    "    encoded = encoded.reshape(*x.shape[:-1], -1)\n",
    "    return torch.cat([x, encoded], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dfebde",
   "metadata": {},
   "source": [
    "## 4. NeRF Model (MLP)\n",
    "\n",
    "Define the simple Multi-Layer Perceptron (MLP) that forms the core of NeRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ea2d7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyNeRF(\n",
      "  (layer1): Linear(in_features=63, out_features=128, bias=True)\n",
      "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (layer3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (layer4): Linear(in_features=128, out_features=129, bias=True)\n",
      "  (layer5): Linear(in_features=155, out_features=64, bias=True)\n",
      "  (layer6): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TinyNeRF(nn.Module):\n",
    "    def __init__(self, pos_dim=3, dir_dim=3, num_pos_freqs=10, num_dir_freqs=4, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.pos_dim = pos_dim\n",
    "        self.dir_dim = dir_dim\n",
    "        self.num_pos_freqs = num_pos_freqs\n",
    "        self.num_dir_freqs = num_dir_freqs\n",
    "\n",
    "        self.encoded_pos_dim = pos_dim * (1 + 2 * num_pos_freqs)\n",
    "        self.encoded_dir_dim = dir_dim * (1 + 2 * num_dir_freqs)\n",
    "\n",
    "        self.layer1 = nn.Linear(self.encoded_pos_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer4 = nn.Linear(hidden_dim, hidden_dim + 1)\n",
    "\n",
    "        self.layer5 = nn.Linear(hidden_dim + self.encoded_dir_dim, hidden_dim // 2)\n",
    "        self.layer6 = nn.Linear(hidden_dim // 2, 3)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, pos, view_dir):\n",
    "        encoded_pos = positional_encoding(pos, self.num_pos_freqs)\n",
    "        encoded_dir = positional_encoding(view_dir, self.num_dir_freqs)\n",
    "\n",
    "        x = self.relu(self.layer1(encoded_pos))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        features = x[..., :-1]\n",
    "        sigma = self.relu(x[..., -1])\n",
    "\n",
    "        x = torch.cat([features, encoded_dir], dim=-1)\n",
    "        x = self.relu(self.layer5(x))\n",
    "        rgb = self.sigmoid(self.layer6(x))\n",
    "\n",
    "        return rgb, sigma\n",
    "\n",
    "# Instantiate the model\n",
    "model = TinyNeRF(hidden_dim=128).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b33ab",
   "metadata": {},
   "source": [
    "## 5. Volume Rendering\n",
    "\n",
    "Implement the volume rendering equation to composite colors and densities along each ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b648bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rays(model, rays_o, rays_d, near=2.0, far=6.0, num_samples=64):\n",
    "    t_vals = torch.linspace(near, far, num_samples, device=device)\n",
    "    points = rays_o[..., None, :] + rays_d[..., None, :] * t_vals[..., :, None]\n",
    "    view_dirs = rays_d[..., None, :].expand(-1, num_samples, -1)\n",
    "\n",
    "    points_flat = points.reshape(-1, 3)\n",
    "    view_dirs_flat = view_dirs.reshape(-1, 3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rgb_flat, sigma_flat = model(points_flat, view_dirs_flat)\n",
    "\n",
    "    rgb = rgb_flat.reshape(*points.shape[:-1], 3)\n",
    "    sigma = sigma_flat.reshape(*points.shape[:-1])\n",
    "\n",
    "    diffs = t_vals[1:] - t_vals[:-1]\n",
    "    dists_1d = torch.cat([diffs, torch.tensor([1e10], device=device)], dim=0)\n",
    "    dists = dists_1d.expand(sigma.shape)\n",
    "\n",
    "    alpha = 1.0 - torch.exp(-sigma * dists)\n",
    "    transmittance = torch.cumprod(torch.cat([torch.ones_like(alpha[:, :1]), 1.0 - alpha + 1e-10], dim=-1), dim=-1)[:, :-1]\n",
    "    weights = transmittance * alpha\n",
    "\n",
    "    rendered_color = torch.sum(weights[..., None] * rgb, dim=-2)\n",
    "\n",
    "    return rendered_color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c412e81",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "Set up the optimizer and run the training loop. We'll train on batches of rays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe5abdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 12000 iterations on 20 views...\n",
      "Total rays: 12800000, Batch size: 1024\n",
      "This will be significantly slower than single-view training on CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43adfdc83f5a4dc68defe83874858bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/12000, Loss: 0.062082\n",
      "Iteration 200/12000, Loss: 0.049168\n",
      "Iteration 200/12000, Loss: 0.049168\n",
      "Iteration 300/12000, Loss: 0.023173\n",
      "Iteration 300/12000, Loss: 0.023173\n",
      "Iteration 400/12000, Loss: 0.017204\n",
      "Iteration 400/12000, Loss: 0.017204\n",
      "Iteration 500/12000, Loss: 0.016035\n",
      "Iteration 500/12000, Loss: 0.016035\n",
      "Iteration 600/12000, Loss: 0.011331\n",
      "Iteration 600/12000, Loss: 0.011331\n",
      "Iteration 700/12000, Loss: 0.012400\n",
      "Iteration 700/12000, Loss: 0.012400\n",
      "Iteration 800/12000, Loss: 0.014174\n",
      "Iteration 800/12000, Loss: 0.014174\n",
      "Iteration 900/12000, Loss: 0.015312\n",
      "Iteration 900/12000, Loss: 0.015312\n",
      "Iteration 1000/12000, Loss: 0.010984\n",
      "Iteration 1000/12000, Loss: 0.010984\n",
      "Iteration 1100/12000, Loss: 0.012238\n",
      "Iteration 1100/12000, Loss: 0.012238\n",
      "Iteration 1200/12000, Loss: 0.011875\n",
      "Iteration 1200/12000, Loss: 0.011875\n",
      "Iteration 1300/12000, Loss: 0.012147\n",
      "Iteration 1300/12000, Loss: 0.012147\n",
      "Iteration 1400/12000, Loss: 0.011922\n",
      "Iteration 1400/12000, Loss: 0.011922\n",
      "Iteration 1500/12000, Loss: 0.011784\n",
      "Iteration 1500/12000, Loss: 0.011784\n",
      "Iteration 1600/12000, Loss: 0.011233\n",
      "Iteration 1600/12000, Loss: 0.011233\n",
      "Iteration 1700/12000, Loss: 0.009942\n",
      "Iteration 1700/12000, Loss: 0.009942\n",
      "Iteration 1800/12000, Loss: 0.009542\n",
      "Iteration 1800/12000, Loss: 0.009542\n",
      "Iteration 1900/12000, Loss: 0.010359\n",
      "Iteration 1900/12000, Loss: 0.010359\n",
      "Iteration 2000/12000, Loss: 0.010743\n",
      "Iteration 2000/12000, Loss: 0.010743\n",
      "Iteration 2100/12000, Loss: 0.009909\n",
      "Iteration 2100/12000, Loss: 0.009909\n",
      "Iteration 2200/12000, Loss: 0.008615\n",
      "Iteration 2200/12000, Loss: 0.008615\n",
      "Iteration 2300/12000, Loss: 0.008175\n",
      "Iteration 2300/12000, Loss: 0.008175\n",
      "Iteration 2400/12000, Loss: 0.009074\n",
      "Iteration 2400/12000, Loss: 0.009074\n",
      "Iteration 2500/12000, Loss: 0.008962\n",
      "Iteration 2500/12000, Loss: 0.008962\n",
      "Iteration 2600/12000, Loss: 0.009765\n",
      "Iteration 2600/12000, Loss: 0.009765\n",
      "Iteration 2700/12000, Loss: 0.008188\n",
      "Iteration 2700/12000, Loss: 0.008188\n",
      "Iteration 2800/12000, Loss: 0.008410\n",
      "Iteration 2800/12000, Loss: 0.008410\n",
      "Iteration 2900/12000, Loss: 0.008205\n",
      "Iteration 2900/12000, Loss: 0.008205\n",
      "Iteration 3000/12000, Loss: 0.009841\n",
      "Iteration 3000/12000, Loss: 0.009841\n",
      "Iteration 3100/12000, Loss: 0.008161\n",
      "Iteration 3100/12000, Loss: 0.008161\n",
      "Iteration 3200/12000, Loss: 0.006751\n",
      "Iteration 3200/12000, Loss: 0.006751\n",
      "Iteration 3300/12000, Loss: 0.008952\n",
      "Iteration 3300/12000, Loss: 0.008952\n",
      "Iteration 3400/12000, Loss: 0.008929\n",
      "Iteration 3400/12000, Loss: 0.008929\n",
      "Iteration 3500/12000, Loss: 0.008033\n",
      "Iteration 3500/12000, Loss: 0.008033\n",
      "Iteration 3600/12000, Loss: 0.007386\n",
      "Iteration 3600/12000, Loss: 0.007386\n",
      "Iteration 3700/12000, Loss: 0.007110\n",
      "Iteration 3700/12000, Loss: 0.007110\n",
      "Iteration 3800/12000, Loss: 0.008577\n",
      "Iteration 3800/12000, Loss: 0.008577\n",
      "Iteration 3900/12000, Loss: 0.007641\n",
      "Iteration 3900/12000, Loss: 0.007641\n",
      "Iteration 4000/12000, Loss: 0.008493\n",
      "Iteration 4000/12000, Loss: 0.008493\n",
      "Iteration 4100/12000, Loss: 0.008251\n",
      "Iteration 4100/12000, Loss: 0.008251\n",
      "Iteration 4200/12000, Loss: 0.007706\n",
      "Iteration 4200/12000, Loss: 0.007706\n",
      "Iteration 4300/12000, Loss: 0.007770\n",
      "Iteration 4300/12000, Loss: 0.007770\n",
      "Iteration 4400/12000, Loss: 0.008427\n",
      "Iteration 4400/12000, Loss: 0.008427\n",
      "Iteration 4500/12000, Loss: 0.006881\n",
      "Iteration 4500/12000, Loss: 0.006881\n",
      "Iteration 4600/12000, Loss: 0.009638\n",
      "Iteration 4600/12000, Loss: 0.009638\n",
      "Iteration 4700/12000, Loss: 0.008876\n",
      "Iteration 4700/12000, Loss: 0.008876\n",
      "Iteration 4800/12000, Loss: 0.007145\n",
      "Iteration 4800/12000, Loss: 0.007145\n",
      "Iteration 4900/12000, Loss: 0.007938\n",
      "Iteration 4900/12000, Loss: 0.007938\n",
      "Iteration 5000/12000, Loss: 0.007140\n",
      "Iteration 5000/12000, Loss: 0.007140\n",
      "Iteration 5100/12000, Loss: 0.005761\n",
      "Iteration 5100/12000, Loss: 0.005761\n",
      "Iteration 5200/12000, Loss: 0.007973\n",
      "Iteration 5200/12000, Loss: 0.007973\n",
      "Iteration 5300/12000, Loss: 0.007989\n",
      "Iteration 5300/12000, Loss: 0.007989\n",
      "Iteration 5400/12000, Loss: 0.006770\n",
      "Iteration 5400/12000, Loss: 0.006770\n",
      "Iteration 5500/12000, Loss: 0.007621\n",
      "Iteration 5500/12000, Loss: 0.007621\n",
      "Iteration 5600/12000, Loss: 0.007902\n",
      "Iteration 5600/12000, Loss: 0.007902\n",
      "Iteration 5700/12000, Loss: 0.007646\n",
      "Iteration 5700/12000, Loss: 0.007646\n",
      "Iteration 5800/12000, Loss: 0.007635\n",
      "Iteration 5800/12000, Loss: 0.007635\n",
      "Iteration 5900/12000, Loss: 0.007489\n",
      "Iteration 5900/12000, Loss: 0.007489\n",
      "Iteration 6000/12000, Loss: 0.006123\n",
      "Iteration 6000/12000, Loss: 0.006123\n",
      "Iteration 6100/12000, Loss: 0.007008\n",
      "Iteration 6100/12000, Loss: 0.007008\n",
      "Iteration 6200/12000, Loss: 0.006277\n",
      "Iteration 6200/12000, Loss: 0.006277\n",
      "Iteration 6300/12000, Loss: 0.009064\n",
      "Iteration 6300/12000, Loss: 0.009064\n",
      "Iteration 6400/12000, Loss: 0.008609\n",
      "Iteration 6400/12000, Loss: 0.008609\n",
      "Iteration 6500/12000, Loss: 0.008526\n",
      "Iteration 6500/12000, Loss: 0.008526\n",
      "Iteration 6600/12000, Loss: 0.006691\n",
      "Iteration 6600/12000, Loss: 0.006691\n",
      "Iteration 6700/12000, Loss: 0.007728\n",
      "Iteration 6700/12000, Loss: 0.007728\n",
      "Iteration 6800/12000, Loss: 0.006167\n",
      "Iteration 6800/12000, Loss: 0.006167\n",
      "Iteration 6900/12000, Loss: 0.004536\n",
      "Iteration 6900/12000, Loss: 0.004536\n",
      "Iteration 7000/12000, Loss: 0.008010\n",
      "Iteration 7000/12000, Loss: 0.008010\n",
      "Iteration 7100/12000, Loss: 0.007215\n",
      "Iteration 7100/12000, Loss: 0.007215\n",
      "Iteration 7200/12000, Loss: 0.005439\n",
      "Iteration 7200/12000, Loss: 0.005439\n",
      "Iteration 7300/12000, Loss: 0.006390\n",
      "Iteration 7300/12000, Loss: 0.006390\n",
      "Iteration 7400/12000, Loss: 0.005367\n",
      "Iteration 7400/12000, Loss: 0.005367\n",
      "Iteration 7500/12000, Loss: 0.005715\n",
      "Iteration 7500/12000, Loss: 0.005715\n",
      "Iteration 7600/12000, Loss: 0.006858\n",
      "Iteration 7600/12000, Loss: 0.006858\n",
      "Iteration 7700/12000, Loss: 0.007301\n",
      "Iteration 7700/12000, Loss: 0.007301\n",
      "Iteration 7800/12000, Loss: 0.005941\n",
      "Iteration 7800/12000, Loss: 0.005941\n",
      "Iteration 7900/12000, Loss: 0.007096\n",
      "Iteration 7900/12000, Loss: 0.007096\n",
      "Iteration 8000/12000, Loss: 0.007343\n",
      "Iteration 8000/12000, Loss: 0.007343\n",
      "Iteration 8100/12000, Loss: 0.007749\n",
      "Iteration 8100/12000, Loss: 0.007749\n",
      "Iteration 8200/12000, Loss: 0.006064\n",
      "Iteration 8200/12000, Loss: 0.006064\n",
      "Iteration 8300/12000, Loss: 0.006604\n",
      "Iteration 8300/12000, Loss: 0.006604\n",
      "Iteration 8400/12000, Loss: 0.006895\n",
      "Iteration 8400/12000, Loss: 0.006895\n",
      "Iteration 8500/12000, Loss: 0.006188\n",
      "Iteration 8500/12000, Loss: 0.006188\n",
      "Iteration 8600/12000, Loss: 0.006299\n",
      "Iteration 8600/12000, Loss: 0.006299\n",
      "Iteration 8700/12000, Loss: 0.006755\n",
      "Iteration 8700/12000, Loss: 0.006755\n",
      "Iteration 8800/12000, Loss: 0.006075\n",
      "Iteration 8800/12000, Loss: 0.006075\n",
      "Iteration 8900/12000, Loss: 0.005830\n",
      "Iteration 8900/12000, Loss: 0.005830\n",
      "Iteration 9000/12000, Loss: 0.006804\n",
      "Iteration 9000/12000, Loss: 0.006804\n",
      "Iteration 9100/12000, Loss: 0.007202\n",
      "Iteration 9100/12000, Loss: 0.007202\n",
      "Iteration 9200/12000, Loss: 0.008853\n",
      "Iteration 9200/12000, Loss: 0.008853\n",
      "Iteration 9300/12000, Loss: 0.005403\n",
      "Iteration 9300/12000, Loss: 0.005403\n",
      "Iteration 9400/12000, Loss: 0.006918\n",
      "Iteration 9400/12000, Loss: 0.006918\n",
      "Iteration 9500/12000, Loss: 0.008496\n",
      "Iteration 9500/12000, Loss: 0.008496\n",
      "Iteration 9600/12000, Loss: 0.007218\n",
      "Iteration 9600/12000, Loss: 0.007218\n",
      "Iteration 9700/12000, Loss: 0.005991\n",
      "Iteration 9700/12000, Loss: 0.005991\n",
      "Iteration 9800/12000, Loss: 0.005538\n",
      "Iteration 9800/12000, Loss: 0.005538\n",
      "Iteration 9900/12000, Loss: 0.005842\n",
      "Iteration 9900/12000, Loss: 0.005842\n",
      "Iteration 10000/12000, Loss: 0.004938\n",
      "Iteration 10000/12000, Loss: 0.004938\n",
      "Iteration 10100/12000, Loss: 0.006355\n",
      "Iteration 10100/12000, Loss: 0.006355\n",
      "Iteration 10200/12000, Loss: 0.006334\n",
      "Iteration 10200/12000, Loss: 0.006334\n",
      "Iteration 10300/12000, Loss: 0.006840\n",
      "Iteration 10300/12000, Loss: 0.006840\n",
      "Iteration 10400/12000, Loss: 0.006023\n",
      "Iteration 10400/12000, Loss: 0.006023\n",
      "Iteration 10500/12000, Loss: 0.005199\n",
      "Iteration 10500/12000, Loss: 0.005199\n",
      "Iteration 10600/12000, Loss: 0.006405\n",
      "Iteration 10600/12000, Loss: 0.006405\n",
      "Iteration 10700/12000, Loss: 0.007132\n",
      "Iteration 10700/12000, Loss: 0.007132\n",
      "Iteration 10800/12000, Loss: 0.005167\n",
      "Iteration 10800/12000, Loss: 0.005167\n",
      "Iteration 10900/12000, Loss: 0.006122\n",
      "Iteration 10900/12000, Loss: 0.006122\n",
      "Iteration 11000/12000, Loss: 0.004665\n",
      "Iteration 11000/12000, Loss: 0.004665\n",
      "Iteration 11100/12000, Loss: 0.005871\n",
      "Iteration 11100/12000, Loss: 0.005871\n",
      "Iteration 11200/12000, Loss: 0.006359\n",
      "Iteration 11200/12000, Loss: 0.006359\n",
      "Iteration 11300/12000, Loss: 0.006507\n",
      "Iteration 11300/12000, Loss: 0.006507\n",
      "Iteration 11400/12000, Loss: 0.006010\n",
      "Iteration 11400/12000, Loss: 0.006010\n",
      "Iteration 11500/12000, Loss: 0.006498\n",
      "Iteration 11500/12000, Loss: 0.006498\n",
      "Iteration 11600/12000, Loss: 0.007226\n",
      "Iteration 11600/12000, Loss: 0.007226\n",
      "Iteration 11700/12000, Loss: 0.004752\n",
      "Iteration 11700/12000, Loss: 0.004752\n",
      "Iteration 11800/12000, Loss: 0.006109\n",
      "Iteration 11800/12000, Loss: 0.006109\n",
      "Iteration 11900/12000, Loss: 0.005493\n",
      "Iteration 11900/12000, Loss: 0.005493\n",
      "Iteration 12000/12000, Loss: 0.005323\n",
      "Training finished.\n",
      "Iteration 12000/12000, Loss: 0.005323\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "learning_rate = 5e-4\n",
    "num_iterations = 12000\n",
    "batch_size = 1024\n",
    "display_rate = 100\n",
    "save_rate = 1000 # Save checkpoint every 1000 iterations\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Check if data loaded successfully before starting training\n",
    "if 'data_loaded_successfully' in locals() and data_loaded_successfully:\n",
    "    print(f'Starting training for {num_iterations} iterations on {num_train_views} views...')\n",
    "    print(f'Total rays: {num_rays}, Batch size: {batch_size}')\n",
    "    print(f'Saving checkpoints every {save_rate} iterations.')\n",
    "    print('This will be significantly slower than single-view training on CPU.')\n",
    "\n",
    "    model.train()\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        indices = torch.randint(0, num_rays, (batch_size,))\n",
    "        batch_rays_o = rays_o_flat[indices].to(device)\n",
    "        batch_rays_d = rays_d_flat[indices].to(device)\n",
    "        batch_target_pixels = target_img_flat[indices].to(device)\n",
    "\n",
    "        t_vals = torch.linspace(2.0, 6.0, 64, device=device)\n",
    "        t_rand = torch.rand(batch_size, 64, device=device) * (t_vals[1] - t_vals[0])\n",
    "        t_vals_noisy = t_vals + t_rand\n",
    "\n",
    "        points = batch_rays_o[..., None, :] + batch_rays_d[..., None, :] * t_vals_noisy[..., :, None]\n",
    "        view_dirs = batch_rays_d[..., None, :].expand(-1, 64, -1)\n",
    "\n",
    "        points_flat = points.reshape(-1, 3)\n",
    "        view_dirs_flat = view_dirs.reshape(-1, 3)\n",
    "\n",
    "        rgb_flat, sigma_flat = model(points_flat, view_dirs_flat)\n",
    "\n",
    "        rgb = rgb_flat.reshape(batch_size, 64, 3)\n",
    "        sigma = sigma_flat.reshape(batch_size, 64)\n",
    "\n",
    "        dists = torch.cat([t_vals_noisy[:, 1:] - t_vals_noisy[:, :-1], torch.tensor([1e10], device=device).expand(batch_size, 1)], dim=-1)\n",
    "        alpha = 1.0 - torch.exp(-sigma * dists)\n",
    "        transmittance = torch.cumprod(torch.cat([torch.ones_like(alpha[:, :1]), 1.0 - alpha + 1e-10], dim=-1), dim=-1)[:, :-1]\n",
    "        weights = transmittance * alpha\n",
    "        rendered_color = torch.sum(weights[..., None] * rgb, dim=-2)\n",
    "\n",
    "        loss = mse_loss(rendered_color, batch_target_pixels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % display_rate == 0:\n",
    "            tqdm.write(f'Iteration {i+1}/{num_iterations}, Loss: {loss.item():.6f}')\n",
    "\n",
    "        # --- Checkpoint Saving ---\n",
    "        if (i + 1) % save_rate == 0:\n",
    "            save_num_views_ckpt = num_train_views if 'num_train_views' in locals() else 'multi'\n",
    "            checkpoint_path = f'tiny_nerf_lego_multi_{save_num_views_ckpt}views_{i+1}iter_{device}.pth'\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            tqdm.write(f'Checkpoint saved to {checkpoint_path}')\n",
    "        # --- End Checkpoint Saving ---\n",
    "\n",
    "    print('Training finished.')\n",
    "else:\n",
    "    print('Skipping training because data loading failed in the previous step.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f1453",
   "metadata": {},
   "source": [
    "## 7. Visualization\n",
    "\n",
    "Render the full image using the trained model and compare it to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "701a8be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load test image: 'NoneType' object is not subscriptable\n",
      "Skipping test image rendering due to loading error.\n"
     ]
    }
   ],
   "source": [
    "# Check if training ran before attempting visualization\n",
    "if 'data_loaded_successfully' in locals() and data_loaded_successfully:\n",
    "    model.eval()\n",
    "\n",
    "    # --- Load a test view for evaluation ---\n",
    "    test_split = 'test'\n",
    "    test_img_idx = 0\n",
    "    test_target_img, test_pose = None, None\n",
    "    try:\n",
    "        with open(os.path.join(data_dir, f'transforms_{test_split}.json'), 'r') as f:\n",
    "            test_meta = json.load(f)\n",
    "        test_frame = test_meta['frames'][test_img_idx]\n",
    "        test_img_path = os.path.join(data_dir, test_frame['file_path'] + '.png')\n",
    "        if H is None or W is None or focal_length is None:\n",
    "             raise ValueError(\"Image dimensions or focal length not loaded correctly.\")\n",
    "        test_target_img = torch.tensor(test_target_img[..., :3] / 255.0, dtype=torch.float32).to(device)\n",
    "        test_pose = torch.tensor(test_frame['transform_matrix'], dtype=torch.float32).to(device)\n",
    "        print(f\"Loaded test image {test_img_idx} for evaluation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load test image: {e}\")\n",
    "    # --- End loading test view ---\n",
    "\n",
    "    if test_target_img is not None and test_pose is not None:\n",
    "        test_rays_o, test_rays_d = get_rays(H, W, focal_length, test_pose)\n",
    "        test_rays_o_flat = test_rays_o.reshape(-1, 3)\n",
    "        test_rays_d_flat = test_rays_d.reshape(-1, 3)\n",
    "        test_num_rays = test_rays_o_flat.shape[0]\n",
    "\n",
    "        rendered_image_flat = []\n",
    "        render_batch_size = 2048\n",
    "\n",
    "        print(f'Rendering test image (using {render_batch_size} rays per batch)...')\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, test_num_rays, render_batch_size)):\n",
    "                batch_rays_o = test_rays_o_flat[i:i+render_batch_size].to(device)\n",
    "                batch_rays_d = test_rays_d_flat[i:i+render_batch_size].to(device)\n",
    "                rendered_batch = render_rays(model, batch_rays_o, batch_rays_d, near=2.0, far=6.0, num_samples=64)\n",
    "                rendered_image_flat.append(rendered_batch)\n",
    "\n",
    "        rendered_image = torch.cat(rendered_image_flat, dim=0).reshape(H, W, 3)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        axes[0].imshow(test_target_img.cpu().numpy())\n",
    "        axes[0].set_title(f'Test Image ({test_img_idx})')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(rendered_image.cpu().numpy())\n",
    "        axes[1].set_title(f'Rendered Test Image (After {num_iterations} iterations)')\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        mse = torch.mean((rendered_image - test_target_img)**2)\n",
    "        psnr = -10.0 * torch.log10(mse)\n",
    "        print(f'Test Image PSNR: {psnr.item():.2f} dB')\n",
    "    else:\n",
    "        print(\"Skipping test image rendering due to loading error.\")\n",
    "else:\n",
    "    print('Skipping visualization because data loading or training failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27293c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to tiny_nerf_lego_multi_20views_12000iterations_cpu.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the final trained model weights\n",
    "if 'data_loaded_successfully' in locals() and data_loaded_successfully:\n",
    "    save_num_views = num_train_views if 'num_train_views' in locals() else 'multi'\n",
    "    model_save_path = f'tiny_nerf_lego_multi_{save_num_views}views_{num_iterations}iterations_final_{device}.pth' # Added _final\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f'Final model weights saved to {model_save_path}')\n",
    "else:\n",
    "    print('Skipping model saving because training did not complete successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a3aa1e",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook provided a minimal NeRF implementation focused on overfitting a single image on the CPU.\n",
    "Key takeaways:\n",
    "*   NeRF combines positional encoding, an MLP, and volume rendering.\n",
    "*   Training even a small model on a single image is slow on CPU.\n",
    "*   The quality depends heavily on model size, number of samples per ray, and training iterations.\n",
    "\n",
    "Further exploration could involve:\n",
    "*   Implementing hierarchical sampling (coarse and fine networks/sampling).\n",
    "*   Training on multiple views to enable novel view synthesis.\n",
    "*   Using a GPU for significantly faster training.\n",
    "*   Exploring libraries like `nerfstudio` for more advanced features and efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
